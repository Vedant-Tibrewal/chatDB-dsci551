{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "from thefuzz import fuzz\n",
    "import networkx as nx\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "from word2number import w2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vedanttibrewal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vedanttibrewal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vedanttibrewal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = {\n",
    "    \"products\": {\n",
    "        \"pk\": [\"product_id\"],\n",
    "        \"fk\": {\"categories\": \"category_id\"},\n",
    "        \"product_id\": \"INT\",\n",
    "        \"product_name\": \"VARCHAR(255)\",\n",
    "        \"description\": \"TEXT\",\n",
    "        \"price\": \"DECIMAL(10, 2)\",\n",
    "        \"stock_quantity\": \"INT\",\n",
    "        \"category_id\": \"INT\"\n",
    "    },\n",
    "    \"categories\": {\n",
    "        \"pk\": [\"category_id\"],\n",
    "        \"fk\": {},\n",
    "        \"category_id\": \"INT\",\n",
    "        \"category_name\": \"VARCHAR(100)\",\n",
    "        \"description\": \"TEXT\"\n",
    "    },\n",
    "    \"customers\": {\n",
    "        \"pk\": [\"customer_id\"],\n",
    "        \"fk\": {},\n",
    "        \"customer_id\": \"INT\",\n",
    "        \"first_name\": \"VARCHAR(50)\",\n",
    "        \"last_name\": \"VARCHAR(50)\",\n",
    "        \"email\": \"VARCHAR(100)\",\n",
    "        \"phone_number\": \"VARCHAR(20)\",\n",
    "        \"address\": \"TEXT\"\n",
    "    },\n",
    "    \"orders\": {\n",
    "        \"pk\": [\"order_id\"],\n",
    "        \"fk\": {\n",
    "            \"customers\": \"customer_id\"\n",
    "        },\n",
    "        \"order_id\": \"INT\",\n",
    "        \"customer_id\": \"INT\",\n",
    "        \"order_date\": \"DATE\",\n",
    "        \"total_amount\": \"DECIMAL(10, 2)\",\n",
    "        \"status\": \"VARCHAR(20)\"\n",
    "    },\n",
    "    \"order_items\": {\n",
    "        \"pk\": [\"order_item_id\"],\n",
    "        \"fk\": {\n",
    "            \"orders\": \"order_id\",\n",
    "            \"products\": \"product_id\"\n",
    "        },\n",
    "        \"order_item_id\":\"INT\",\n",
    "        \"order_id\": \"INT\",\n",
    "        \"product_id\": \"INT\",\n",
    "        \"quantity\": \"INT\",\n",
    "        \"unit_price\": \"DECIMAL(10, 2)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "req = {\n",
    "    \"products\": [\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"price\"\n",
    "    ],\n",
    "    \"customers\": [\n",
    "        \"customer_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "        \"email\"\n",
    "    ],\n",
    "    \"orders\": [\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_date\",\n",
    "        \"total_amount\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Healthcare Schema\n",
    "\n",
    "SQL = {\n",
    "    \"admissions\": {\n",
    "        \"pk\": [\"admissionid\"],\n",
    "        \"fk\": {\n",
    "            \"patients\": \"patientid\",\n",
    "            \"insurance\": \"insuranceid\"\n",
    "        },\n",
    "        \"admissionid\": \"INT\",\n",
    "        \"patientid\": \"INT\",\n",
    "        \"insuranceid\": \"INT\",  \n",
    "        \"doctor\": \"VARCHAR(255)\",\n",
    "        \"hospital\": \"VARCHAR(255)\",\n",
    "        \"intakedate\": \"DATE\",\n",
    "        \"dischargedate\": \"DATE\",\n",
    "        \"roomnumber\": \"INT\",\n",
    "        \"carelevel\": \"VARCHAR(50)\"\n",
    "    },\n",
    "\n",
    "    \"patients\": {\n",
    "        \"pk\": [\"patientid\"],\n",
    "        \"fk\": {},\n",
    "        \"patientid\": \"INT\",\n",
    "        \"patientname\": \"VARCHAR(255)\",\n",
    "        \"age\": \"INT\",\n",
    "        \"gender\": \"VARCHAR(10)\",\n",
    "        \"bloodtype\": \"VARCHAR(5)\",\n",
    "        \"disease\": \"VARCHAR(255)\"\n",
    "    },\n",
    "\n",
    "    \"insurance\": {\n",
    "        \"pk\": [\"insuranceid\"],\n",
    "        \"fk\": {\n",
    "            \"patients\": \"patientid\"\n",
    "        },\n",
    "        \"insuranceid\": \"INT\",\n",
    "        \"patientid\": \"INT\",\n",
    "        \"insuranceprovider\": \"VARCHAR(255)\",\n",
    "        \"billingcost\": \"DECIMAL(10, 2)\",\n",
    "        \"medication\": \"VARCHAR(255)\",\n",
    "        \"testresults\": \"VARCHAR(50)\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic COUNT query\n",
    "q1 = \"What is the total number of customers?\"\n",
    "sql1 = \"SELECT COUNT(*) FROM customers;\"\n",
    "\n",
    "# Simple SELECT with WHERE clause\n",
    "q2 = \"List all products names with a price greater than $50.\"\n",
    "sql2 = \"SELECT product_name, price FROM products WHERE price > 50;\"\n",
    "\n",
    "# JOIN operation with ORDER BY\n",
    "q3 = \"Show the first name of customers who have placed orders, sorted by their last name.\"\n",
    "sql3 = \"\"\"\n",
    "SELECT DISTINCT c.first_name, c.last_name \n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "ORDER BY c.last_name;\n",
    "\"\"\"\n",
    "\n",
    "# Aggregation with GROUP BY\n",
    "q4 = \"What is the total amount of orders for each customer?\"\n",
    "sql4 = \"\"\"\n",
    "SELECT c.customer_id, c.first_name, c.last_name, SUM(o.total_amount) as total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.first_name, c.last_name;\n",
    "\"\"\"\n",
    "\n",
    "# HAVING clause\n",
    "q5 = \"Which categories have count more than 5 products?\"\n",
    "sql5 = \"\"\"\n",
    "SELECT c.category_name, COUNT(p.product_id) as product_count\n",
    "FROM categories c\n",
    "JOIN products p ON c.category_id = p.category_id\n",
    "GROUP BY c.category_name\n",
    "HAVING COUNT(p.product_id) > 5;\n",
    "\"\"\"\n",
    "\n",
    "# LIMIT clause\n",
    "q6 = \"What are the top 5 most priciest products?\" # \"What are the top 5 most expensive products?\"\n",
    "sql6 = \"\"\"\n",
    "SELECT product_name, price\n",
    "FROM products\n",
    "ORDER BY price DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "# Aggregation in WHERE clause\n",
    "q7 = \"Find orders with more than 3 product items.\"\n",
    "sql7 = \"\"\"\n",
    "SELECT o.order_id, COUNT(oi.product_id) as item_count\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "GROUP BY o.order_id\n",
    "HAVING COUNT(oi.product_id) > 3;\n",
    "\"\"\"\n",
    "\n",
    "# Multiple JOINs\n",
    "q8 = \"List the top 3 customers who have spent the most on 'Electronics' category products.\"\n",
    "sql8 = \"\"\"\n",
    "SELECT c.customer_id, c.first_name, c.last_name, SUM(oi.quantity * oi.unit_price) as total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "JOIN categories cat ON p.category_id = cat.category_id\n",
    "WHERE cat.category_name = 'Electronics'\n",
    "GROUP BY c.customer_id, c.first_name, c.last_name\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 3;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'the',\n",
    "    'i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'we', 'our', 'ours', 'ourselves', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'do', 'does', 'did', 'have', 'has', 'had', 'can', 'could', 'shall', 'should', 'will', 'would', 'may', 'might', 'must',\n",
    "    'about', 'across', 'after', 'against', 'along', 'around', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'during', 'into', 'near', 'off', 'out', 'through', 'toward', 'under', 'up', 'with',\n",
    "    'if', 'then', 'because', 'as', 'until', 'while',\n",
    "    'this', 'that', 'these', 'those', 'such', 'what', 'which', 'whose', 'whoever', 'whatever', 'whichever', 'whomever', 'either', 'neither', 'both',\n",
    "    'very', 'really', 'always', 'never', 'too', 'already', 'often', 'sometimes', 'rarely', 'seldom', 'again', 'further', 'then', 'once', 'here', 'there', 'where', 'why', 'how',\n",
    "    # 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'second', 'third', 'fourth', 'fifth', # find a way to implement it in limit\n",
    "    'few', 'little', 'much', 'enough',\n",
    "    'yes', 'no', 'not', 'okay', 'ok', 'right', 'sure', 'well', 'uh', 'um', 'oh', 'eh', 'hmm', 'just', 'ever', 'yet', 'etc', 'perhaps', 'maybe', 'list',\n",
    "    'who', 'spent'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aggregate_functions = {\n",
    "    'COUNT': ['count', 'number of', 'quantity of', 'total number of', 'tally', 'enumerate', 'how many'],\n",
    "    'SUM': ['sum', 'total', 'aggregate', 'combined', 'add up', 'overall', 'cumulative'],\n",
    "    'AVG': ['average', 'mean', 'typical', 'median', 'expected value', 'norm', 'central tendency'],\n",
    "    'MAX': ['maximum', 'highest', 'most', 'top', 'peak', 'greatest', 'largest', 'biggest', 'uppermost'],\n",
    "    'MIN': ['minimum', 'lowest', 'least', 'bottom', 'smallest', 'tiniest', 'least', 'floor'],\n",
    "    'DISTINCT': ['unique', 'different', 'distinct', 'individual', 'separate', 'non-duplicate', 'exclusive'],\n",
    "    # 'GROUP_CONCAT': ['concatenate', 'combine strings', 'join', 'merge text', 'string aggregation', 'text combination'],\n",
    "    # 'FIRST': ['first', 'initial', 'earliest', 'primary', 'leading', 'opening', 'foremost'], # limit implementation along with number\n",
    "    # 'LAST': ['last', 'final', 'latest', 'ultimate', 'concluding', 'terminal', 'closing']\n",
    "}\n",
    "\n",
    "comparison_operators = {\n",
    "    '>': ['with greater than', 'greater than', 'with more than', 'more than', 'is above' ,'above', 'over', 'exceeding', 'surpassing', 'beyond', 'higher than', 'in excess of'],\n",
    "    '<': ['with less than', 'less than', 'with fewer than', 'fewer than', 'is below' ,'below', 'under', 'beneath', 'lower than', 'not as much as', 'smaller than'],\n",
    "    '=': ['equal to', 'is same as', 'same as', 'identical to', 'matching', 'equivalent to', 'corresponds to', 'is', 'for', 'with'],\n",
    "    '!=': ['not equal to', 'different from', 'excluding', 'not the same as', 'dissimilar to', 'unlike', 'other than'],\n",
    "    '>=': ['greater than or equal to', 'at least', 'no less than', 'minimum of', 'not below', ' starting from'],\n",
    "    '<=': ['less than or equal to', 'at most', 'no more than', 'maximum of', 'not above', 'up to'],\n",
    "    'BETWEEN': ['between', 'in the range of', 'within the bounds of', 'inside the limits of'],\n",
    "    'IN': ['in', 'within', 'among', 'included in', 'part of', 'contained in', 'one of'],\n",
    "    'NOT IN': ['not in', 'outside of', 'excluded from', 'not among', 'not part of', 'not contained in'],\n",
    "    'LIKE': ['like', 'similar to', 'resembling', 'matching pattern', 'corresponding to'],\n",
    "    'NOT LIKE': ['not like', 'dissimilar to', 'unlike', 'not matching pattern', 'different from pattern']\n",
    "}\n",
    "\n",
    "logical_operators = {\n",
    "    'AND': ['and', 'also', 'as well as', 'in addition to', 'plus', 'together with', 'along with', 'including'],\n",
    "    'OR': ['or', 'alternatively', 'either', 'otherwise', 'else', 'and/or'],\n",
    "    'NOT': ['not', 'except', 'excluding', 'other than', 'but not', 'save for', 'apart from']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = {\n",
    "    'INTEGER': ['integer', 'int', 'whole number', 'numeric'],\n",
    "    'FLOAT': ['float', 'decimal', 'real number', 'fractional number'],\n",
    "    'VARCHAR': ['string', 'text', 'characters', 'alphanumeric'],\n",
    "    'DATE': ['date', 'calendar day', 'day'],\n",
    "    'TIMESTAMP': ['timestamp', 'date and time', 'moment', 'point in time'],\n",
    "    'BOOLEAN': ['boolean', 'true/false', 'yes/no', 'binary']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order matters\n",
    "sql_clauses = {\n",
    "    'FROM': ['from', 'in', 'out of', 'sourced from', 'derived from', 'based on'],\n",
    "    'WHERE': ['where', 'for which', 'that have', 'meeting the condition', 'satisfying', 'fulfilling'],\n",
    "    'ORDER BY': ['order by', 'ordered by', 'sort by', 'sorted by', 'arrange by', 'rank by', 'sequence by'],\n",
    "    'GROUP BY': ['group by', 'categorize by', 'classify by', 'organize by', 'arrange by', 'cluster by', 'for each', 'broken down by', 'per', 'by'], # check about 'by'\n",
    "    'HAVING': ['having', 'with the condition', 'subject to', 'meeting the criteria', 'have'],\n",
    "    'LIMIT': ['limit', 'top', 'first', 'restrict to', 'cap at', 'only show'],\n",
    "    'JOIN': ['join', 'combine', 'merge', 'connect', 'link', 'associate'],\n",
    "    'UNION': ['union', 'combine', 'merge', 'incorporate', 'consolidate', 'unite'],\n",
    "    'INTERSECT': ['intersect', 'in common', 'shared by', 'mutual', 'overlapping'],\n",
    "    'EXCEPT': ['except', 'subtract', 'exclude', 'remove', 'omit', 'leave out'],\n",
    "    'SELECT': ['show', 'list all', 'list', 'give', 'return', 'fetch', 'retrieve', 'get', 'find', 'which', 'what is the', 'what is', 'what are', 'what'], # 'display\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_KEYWORDS = set(list(aggregate_functions.keys())) | (set(list(logical_operators.keys()))) | (set(sql_clauses.keys())) | (set(comparison_operators.keys()))\n",
    "ALL_KEYWORDS = list(map(str.lower, ALL_KEYWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\"ALL_KEYWORDS\": ALL_KEYWORDS,\n",
    "             \"sql_clauses\": sql_clauses,\n",
    "             \"data_types\": data_types,\n",
    "             \"logical_operators\": logical_operators,\n",
    "             \"comparison_operators\": comparison_operators,\n",
    "             \"aggregate_functions\": aggregate_functions,\n",
    "             \"stop_words\": stop_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_string(text, replace_dict):\n",
    "    # Create a reverse mapping from values to keys\n",
    "    value_to_key = {}\n",
    "    for key, values in replace_dict.items():\n",
    "        for value in values:\n",
    "            value_to_key[value.lower()] = key.lower()\n",
    "    \n",
    "    # Create a regex pattern for matching words\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(word) for word in value_to_key.keys()) + r')\\b'\n",
    "    \n",
    "    # Function to replace matched words\n",
    "    def replace_word(match):\n",
    "        return value_to_key.get(match.group(0).lower(), match.group(0))\n",
    "    \n",
    "    # Perform the replacement\n",
    "    replaced_text = re.sub(pattern, replace_word, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return replaced_text\n",
    "\n",
    "def replace_numbers(token):\n",
    "    try:\n",
    "        return w2n.word_to_num(token)\n",
    "    except:\n",
    "        return token\n",
    "\n",
    "def preprocess_text(text, constants):\n",
    "    # Lowercase\n",
    "    # text = text.lower() # change it to lower while comparisson\n",
    "    \n",
    "    # Remove punctuation\n",
    "    # pattern = r'[^\\w\\s()*\\d-]|(?<!\\d)-(?!\\d)'\n",
    "    # text = re.sub(pattern, '', text)\n",
    "    text = re.sub(r'\\?', '', text)\n",
    "    text = re.sub(r'\\$', '', text)\n",
    "    text = re.sub(r'\\.$', '', text)\n",
    "    print(text)\n",
    "    text = re.sub(r',', '', text)\n",
    "    pattern = r'\\b(\\d+)-(\\d+)\\b'\n",
    "    text = re.sub(pattern, r'\\1 to \\2', text)\n",
    "\n",
    "\n",
    "\n",
    "    # \"name\" condition\n",
    "    tokens = text.split()\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens) - 1:  # Changed to len(tokens) - 1\n",
    "        if tokens[i].lower() not in constants['ALL_KEYWORDS'] and (tokens[i+1].lower() == \"name\" or tokens[i+1].lower() == \"names\"):\n",
    "            # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    if i < len(tokens):  # Add any remaining token\n",
    "        result.append(tokens[i])\n",
    "\n",
    "    i = 0\n",
    "    result2 = []\n",
    "    # group_by_keys = constants['sql_clauses']['GROUP BY']\n",
    "    # group_by_keys = [key.split('by')[0].strip() for key in group_by_keys if key.split('by')[0] != '']\n",
    "    # print(\"group keys\", group_by_keys)\n",
    "\n",
    "    order_by_keys = constants['sql_clauses']['ORDER BY']\n",
    "    order_by_keys = [key.split('by')[0].strip() for key in order_by_keys if key.split('by')[0] != '']\n",
    "    # print(\"order keys\", order_by_keys)\n",
    "\n",
    "    while i < len(result) - 1:\n",
    "        if result[i+1].lower() =='by' and result[i].lower() in order_by_keys: #(result[i].lower() in group_by_keys):\n",
    "            result2.append(f\"{result[i]} {result[i+1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            result2.append(result[i])\n",
    "            i += 1\n",
    "    if i < len(result):  # Add any remaining token\n",
    "        result2.append(result[i])\n",
    "\n",
    "    # print(result)\n",
    "    text = ' '.join(result2)\n",
    "\n",
    "    # substitute words\n",
    "    text = replace_string(text, sql_clauses)\n",
    "    text = replace_string(text, aggregate_functions)\n",
    "    # from \\d+ -> >=\n",
    "    text = replace_string(text, comparison_operators)\n",
    "    between_pattern = r'from \\b(\\w+|\\d+)\\b to \\b(\\w+|\\d+)\\b | \\b(\\w+|\\d+)\\b to \\b(\\w+|\\d+)\\b'\n",
    "    text = re.sub(between_pattern, r\" between(\\1, \\2)\", text)\n",
    "    text = replace_string(text, logical_operators)\n",
    "\n",
    "    # Tokenization\n",
    "    # tokens = word_tokenize(text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    tokens = [str(replace_numbers(token)) for token in tokens]\n",
    "    # print(tokens)\n",
    "\n",
    "    # Remove stopwords\n",
    "    #  load custom stop words\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "\n",
    "    # Lemmatization (optional) // important for similarity\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens# ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triggered sorted by\n",
      "['Show', 'the', 'first_name', 'of', 'customers', 'who', 'have', 'placed', 'orders', 'sorted by']\n",
      "What is the total number of customers?\n",
      "List all products names with a price greater than $50.\n",
      "What is the total amount of orders for each customer?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['select', 'count', 'customer'],\n",
       " ['select', 'products_names', 'price', '>', '50'],\n",
       " ['select',\n",
       "  'first_name',\n",
       "  'of',\n",
       "  'customer',\n",
       "  'placed',\n",
       "  'order',\n",
       "  'order',\n",
       "  'by',\n",
       "  'last_name'],\n",
       " ['select', 'sum', 'amount', 'of', 'order', 'group', 'by', 'custome'])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_tok = preprocess_text(q1, constants)\n",
    "q2_tok = preprocess_text(q2, constants)\n",
    "q3_tok = preprocess_text(q3, constants)\n",
    "q4_tok = preprocess_text(q4, constants)\n",
    "print(q1)\n",
    "print(q2)\n",
    "print(q4)\n",
    "q1_tok, q2_tok, q3_tok, q4_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keywords(tokens, keywords):\n",
    "    token_without_key = [token for token in tokens if token not in keywords]\n",
    "\n",
    "    return token_without_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['customer'],\n",
       " ['products_names', 'price', '50'],\n",
       " ['amount', 'of', 'order', 'group', 'by', 'custome'])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_nk_tok = remove_keywords(q1_tok, ALL_KEYWORDS)\n",
    "q2_nk_tok = remove_keywords(q2_tok, ALL_KEYWORDS)\n",
    "q4_nk_tok = remove_keywords(q4_tok, ALL_KEYWORDS)\n",
    "q1_nk_tok, q2_nk_tok, q4_nk_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orders']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'orders': ['status']}"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indentify_col_tables(identify_group_by(remove_keywords(preprocess_text(\"total orders by status\", constants), ALL_KEYWORDS), SQL), SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_group_by(tokens, schema):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens) - 2:  # Changed to len(tokens) - 1\n",
    "\n",
    "        if tokens[i].lower() == \"group\" and tokens[i+1].lower() == \"by\":\n",
    "            # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            for table, columns in schema.items():\n",
    "                # for column in schema[table].keys():\n",
    "                # for column in columns.keys():\n",
    "                match = get_close_matches(tokens[i+2], list(columns.keys()))\n",
    "                if match:\n",
    "                    # print(tokens[i+2], \":\", match)                        \n",
    "                    result.append(f\"{tokens[i]} {tokens[i+1]} {match[0]}\")\n",
    "                    break\n",
    "            i += 3\n",
    "        else:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    if i < len(tokens):  # Add any remaining token\n",
    "        result.extend(tokens[i:])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total number of customers?\n",
      "['customer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['customer']"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q1)\n",
    "q1_nk_tok = remove_keywords(q1_tok, ALL_KEYWORDS)\n",
    "print(q1_nk_tok)\n",
    "group_tok1 = identify_group_by(q1_nk_tok, SQL)\n",
    "group_tok1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all products names with a price greater than $50.\n",
      "['products_names', 'price', '50']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['products_names', 'price', '50']"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q2)\n",
    "q2_nk_tok = remove_keywords(q2_tok, ALL_KEYWORDS)\n",
    "print(q2_nk_tok)\n",
    "group_tok2 = identify_group_by(q2_nk_tok, SQL)\n",
    "group_tok2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the first name of customers who have placed orders, sorted by their last name.\n",
      "['first_name', 'of', 'customer', 'placed', 'order', 'order', 'by', 'last_name']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first_name', 'of', 'customer', 'placed', 'order', 'order', 'by', 'last_name']"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q3)\n",
    "q3_nk_tok = remove_keywords(q3_tok, ALL_KEYWORDS)\n",
    "print(q3_nk_tok)\n",
    "group_tok3 = identify_group_by(q3_nk_tok, SQL)\n",
    "group_tok3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total amount of orders for each customer?\n",
      "['amount', 'of', 'order', 'group', 'by', 'custome']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amount', 'of', 'order', 'group by customer_id']"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q4)\n",
    "q4_nk_tok = remove_keywords(q4_tok, ALL_KEYWORDS)\n",
    "print(q4_nk_tok)\n",
    "group_tok4 = identify_group_by(q4_nk_tok, SQL)\n",
    "group_tok4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Columns and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by before this\n",
    "def indentify_table(tokens, schema):\n",
    "    \n",
    "    detected_table = [[], [], []] # [[table names], [similarities], [tokens]]\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() == \"from\":\n",
    "            table_name = tokens[i + 1]\n",
    "            if table_name in schema:\n",
    "                print(f\"Table identified: {table_name}\")\n",
    "                return table_name\n",
    "        \n",
    "        for table in schema.keys():\n",
    "            similarity = fuzz.ratio(token, table.lower())\n",
    "            if token in detected_table[2]:\n",
    "                # print(f\"Table identified: {token} {table} {similarity}\")\n",
    "                # print(detected_table)\n",
    "                i = detected_table[2].index(token)\n",
    "                if similarity >= detected_table[1][i]:\n",
    "                    # detected_table[0] = table\n",
    "                    detected_table[1][i] = similarity\n",
    "                    detected_table[0][i] = table\n",
    "                # print(f\"Table identified: {table} {similarity}\")\n",
    "            else:\n",
    "                if similarity >= 70: # initial threshold\n",
    "                    detected_table[0].append(table)\n",
    "                    detected_table[1].append(similarity)\n",
    "                    detected_table[2].append(token)\n",
    "\n",
    "\n",
    "    for i in range(len(detected_table[0])):\n",
    "        if detected_table[1][i] > 85:\n",
    "            tokens.remove(detected_table[2][i])    \n",
    "        \n",
    "    # print(\"dsfsdgd\",detected_table[0])\n",
    "\n",
    "    return detected_table[0]\n",
    "\n",
    "\n",
    "def indentify_col_tables(tokens, schema):\n",
    "    res = dict()\n",
    "\n",
    "    identified_table = indentify_table(tokens, schema)\n",
    "    print(identified_table)\n",
    "    if not tokens:\n",
    "        res[identified_table[0]] = ['*']\n",
    "        return res\n",
    "\n",
    "    if identified_table:\n",
    "        for table in identified_table:\n",
    "            res[table] = dict()\n",
    "            for token in tokens:\n",
    "                for column in schema[table].keys():\n",
    "                    # for column in columns.keys():\n",
    "                    if column=='pk' or column=='fk':\n",
    "                        continue\n",
    "                    else:\n",
    "                        similarity = fuzz.ratio(token, column.lower())\n",
    "                        if similarity >= 50:\n",
    "                            if res[table].get(token):\n",
    "                                # print(\"case 1\")\n",
    "                                # print(token, \":\", column, similarity)\n",
    "                                old_similarity = list(res[table][token].values())[0]\n",
    "                                # print(\"sim\", old_similarity)\n",
    "                                if similarity > old_similarity:\n",
    "                                    res[table][token] = {column: similarity}\n",
    "                            else:\n",
    "                                # print(\"case 2\")\n",
    "                                # print(token, \":\", column, similarity)\n",
    "                                res[table][token] = {column: similarity}\n",
    "                            # print(res)\n",
    "                            # if res.get(identified_table):\n",
    "                            #     res[identified_table].append(column)\n",
    "                            # else:\n",
    "                            #     res[identified_table] = [column]\n",
    "    else:\n",
    "        for token in tokens:\n",
    "            for table, columns in schema.items():\n",
    "                for column in schema[table].keys():\n",
    "                # for column in columns.keys():\n",
    "                    if column=='pk' or column=='fk':\n",
    "                        continue\n",
    "                    else:\n",
    "                        match = get_close_matches(token, columns)\n",
    "                        if match:\n",
    "                            print(\"*\"*4, token, \":\", match[0])\n",
    "                            # if token in columns: # replace by fuzzy logic\n",
    "                            # print(f\"Column identified: {column} in table {table}\")\n",
    "                            if res.get(table):\n",
    "                                res[table].append(match[0])\n",
    "                            else:\n",
    "                                res[table] = [match[0]]\n",
    "\n",
    "    if identified_table:\n",
    "        result = {}\n",
    "        for table, token in res.items():\n",
    "            cols = []\n",
    "            for col in token.values():\n",
    "                cols.extend(col.keys())\n",
    "            result[table] = cols\n",
    "\n",
    "        result = {key: value for key, value in result.items() if value != []}\n",
    "        return result\n",
    "    \n",
    "\n",
    "    res = {key: value for key, value in res.items() if value != []}    \n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all products names with a price greater than $50.\n",
      "['products_names', 'price', '50']\n",
      "['products']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'products': ['product_name', 'price']}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q2)\n",
    "print(q2_nk_tok)\n",
    "q2_nk_tok = remove_keywords(q2_tok, ALL_KEYWORDS)\n",
    "req = indentify_col_tables(group_tok2, SQL)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total number of customers?\n",
      "['customer']\n",
      "['customers']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'customers': ['*']}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q1)\n",
    "print(q1_nk_tok)\n",
    "group_tok1 = identify_group_by(q1_nk_tok, SQL)\n",
    "req = indentify_col_tables(group_tok1, SQL)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total amount of orders for each customer?\n",
      "['amount', 'of', 'order', 'group', 'by', 'custome']\n",
      "['orders']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'orders': ['total_amount', 'customer_id']}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q4)\n",
    "q4_nk_tok = remove_keywords(q4_tok, ALL_KEYWORDS)\n",
    "print(q4_nk_tok)\n",
    "\n",
    "group_tok4 = identify_group_by(q4_nk_tok, SQL)\n",
    "req = indentify_col_tables(group_tok4, SQL)\n",
    "req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(db_schema, directional=False):\n",
    "    if directional:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "\n",
    "    # Add nodes and edges based on the SQL dictionary\n",
    "    for table, details in db_schema.items():\n",
    "        G.add_node(table)  # Add table as a node\n",
    "        for fk_table in details['fk']:  # Iterate through foreign keys\n",
    "            G.add_edge(table, fk_table)  # Create an edge from current table to foreign key table\n",
    "\n",
    "    # Get nodes and edges for verification\n",
    "    nodes = list(G.nodes)\n",
    "    edges = list(G.edges)\n",
    "\n",
    "    # Output nodes and edges\n",
    "    # print(nodes)\n",
    "    # print(edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def required_tables_graph(G, start, end, required_tables):\n",
    "    def dijkstra(graph, start, end):\n",
    "        distances = {node: float('infinity') for node in graph}\n",
    "        distances[start] = 0\n",
    "        pq = [(0, start)]\n",
    "        previous = {node: None for node in graph}\n",
    "\n",
    "        while pq:\n",
    "            current_distance, current_node = heapq.heappop(pq)\n",
    "\n",
    "            if current_node == end:\n",
    "                path = []\n",
    "                while current_node:\n",
    "                    path.append(current_node)\n",
    "                    current_node = previous[current_node]\n",
    "                return path[::-1], current_distance\n",
    "\n",
    "            for neighbor in graph[current_node]:\n",
    "                distance = current_distance + 1  # All edges have weight of 1\n",
    "                if distance < distances[neighbor]:\n",
    "                    distances[neighbor] = distance\n",
    "                    previous[neighbor] = current_node\n",
    "                    heapq.heappush(pq, (distance, neighbor))\n",
    "\n",
    "        return None, float('infinity')\n",
    "\n",
    "    required_tables = set(required_tables) - {start, end}\n",
    "    best_path = None\n",
    "    best_distance = float('infinity')\n",
    "\n",
    "    def dfs(current_path, current_distance, remaining_required):\n",
    "        nonlocal best_path, best_distance\n",
    "\n",
    "        if not remaining_required:\n",
    "            path, distance = dijkstra(G, current_path[-1], end)\n",
    "            if path:\n",
    "                total_path = current_path + path[1:]\n",
    "                total_distance = current_distance + distance\n",
    "                if total_distance < best_distance:\n",
    "                    best_path = total_path\n",
    "                    best_distance = total_distance\n",
    "            return\n",
    "\n",
    "        for node in remaining_required:\n",
    "            path, distance = dijkstra(G, current_path[-1], node)\n",
    "            if path:\n",
    "                new_path = current_path + path[1:]\n",
    "                new_distance = current_distance + distance\n",
    "                new_remaining = remaining_required - {node}\n",
    "                dfs(new_path, new_distance, new_remaining)\n",
    "\n",
    "    dfs([start], 0, required_tables)\n",
    "\n",
    "    return best_path, best_distance\n",
    "\n",
    "# G = create_graph(SQL, directional=False)\n",
    "# # Set start, end, and required nodes\n",
    "# required_tables = ['categories', 'products', 'orders']\n",
    "# start = required_tables[0]\n",
    "# end = required_tables[2]\n",
    "\n",
    "# # Find the shortest path\n",
    "# required_tables, distance = required_tables_graph(G, start, end, required_tables)\n",
    "\n",
    "# print(f\"Shortest path: {' -> '.join(required_tables) if required_tables else 'No path found'}\")\n",
    "# # Shortest path: categories -> products -> order_items -> orders\n",
    "\n",
    "# print(f\"Total distance: {distance}\")\n",
    "# # Total distance: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('products', 'categories'),\n",
       " ('orders', 'customers'),\n",
       " ('order_items', 'orders'),\n",
       " ('order_items', 'products')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = create_graph(SQL, directional=True)\n",
    "list(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_sort(edges):\n",
    "    counts = Counter(t[0] for t in edges)\n",
    "    \n",
    "    # Sort tuples based on the counts in descending order\n",
    "    sorted_tuples = sorted(edges, key=lambda x: counts[x[0]], reverse=True)\n",
    "    \n",
    "    return sorted_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_items', 'orders'),\n",
       " ('order_items', 'products'),\n",
       " ('products', 'categories'),\n",
       " ('orders', 'customers')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_sort(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_clause(req_schema: dict, db_schema: dict):\n",
    "    clause = \"\"\n",
    "\n",
    "    # not required\n",
    "    # max_cols = 0\n",
    "    # prim_table = None\n",
    "    # # change logic based on foreign key numbers\n",
    "    # for table, cols in req_schema.items():\n",
    "    #     if len(cols) > max_cols:\n",
    "    #         max_cols = len(cols)\n",
    "    #         prim_table = table\n",
    "    \n",
    "    # # pk_col = db_schema[prim_table]['pk']\n",
    "    # print(prim_table)\n",
    "\n",
    "    # create graph from Database schema\n",
    "    db_graph = create_graph(db_schema)\n",
    "    db_dir_graph = create_graph(db_schema, directional=True)\n",
    "\n",
    "    # print(db_dir_graph)\n",
    "\n",
    "    required_tables = list(req_schema.keys())\n",
    "\n",
    "    print(required_tables)\n",
    "\n",
    "    min_dist = float('inf')\n",
    "    for st_table in required_tables:\n",
    "        for end_table in required_tables:\n",
    "            sub_graph, distance = required_tables_graph(db_graph, st_table, end_table, required_tables)\n",
    "            if distance < min_dist:\n",
    "                join_graph = OrderedSet(sub_graph)\n",
    "                min_dist = distance\n",
    "\n",
    "    print(join_graph)\n",
    "\n",
    "    req_graph = []\n",
    "\n",
    "    for edge in db_dir_graph.edges:\n",
    "        # assumption binary relations between tables\n",
    "        if edge[0] in join_graph and edge[1] in join_graph:\n",
    "            req_graph.append(edge)\n",
    "\n",
    "    req_graph = graph_sort(req_graph)\n",
    "    print(\"required graph: \", req_graph)\n",
    "\n",
    "    clause += f\"FROM {req_graph[0][0]}\\n\"\n",
    "\n",
    "    for i, edge in enumerate(req_graph):\n",
    "        table1 = edge[0]\n",
    "        table2 = edge[1]\n",
    "        print(edge)\n",
    "        # if table1 in db_schema[table2]['fk']:\n",
    "        #     # print(\"1\")\n",
    "        #     fk_col = db_schema[table2]['fk'][table1] # foreign key corresponding to 2nd table  primary key\n",
    "        #     pk_col = db_schema[table1]['pk'][0] # primary key of 2nd table\n",
    "        #     clause += f\"JOIN {table1} ON {table2}.{fk_col}={table1}.{pk_col} \\n\"\n",
    "        # else:\n",
    "        fk_col = db_schema[table1]['fk'][table2] # foreign key corresponding to 2nd table  primary key\n",
    "        pk_col = db_schema[table2]['pk'][0] # primary key of 2nd table\n",
    "        clause += f\"JOIN {table2} ON {table1}.{fk_col}={table2}.{pk_col} \\n\"\n",
    "        \n",
    "    return clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orders': ['total_amount'], 'customers': []}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orders', 'customers']\n",
      "OrderedSet(['orders', 'customers'])\n",
      "required graph:  [('orders', 'customers')]\n",
      "('orders', 'customers')\n",
      "FROM orders\n",
      "JOIN customers ON orders.customer_id=customers.customer_id \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if len(req.keys()) > 1:\n",
    "    print(join_clause(req, SQL))\n",
    "else:\n",
    "    print(\"No Join required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['books', 'authors']\n",
      "OrderedSet(['books', 'authors'])\n",
      "required graph:  [('books', 'authors')]\n",
      "('books', 'authors')\n",
      "FROM books\n",
      "JOIN authors ON books.author_id=authors.author_id \n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQL2 = {\n",
    "    \"books\": {\n",
    "        \"pk\": [\"book_id\"],\n",
    "        \"fk\": {\"authors\": \"author_id\"},\n",
    "        \"book_id\": \"INT\",\n",
    "        \"title\": \"VARCHAR(255)\",\n",
    "        \"author_id\": \"INT\",\n",
    "        \"isbn\": \"VARCHAR(13)\",\n",
    "        \"publication_year\": \"INT\",\n",
    "        \"price\": \"DECIMAL(10, 2)\",\n",
    "        \"stock_quantity\": \"INT\"\n",
    "    },\n",
    "    \"authors\": {\n",
    "        \"pk\": [\"author_id\"],\n",
    "        \"fk\": {},\n",
    "        \"author_id\": \"INT\",\n",
    "        \"first_name\": \"VARCHAR(50)\",\n",
    "        \"last_name\": \"VARCHAR(50)\",\n",
    "        \"birth_date\": \"DATE\",\n",
    "        \"nationality\": \"VARCHAR(50)\",\n",
    "        \"biography\": \"TEXT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "req2 = {\n",
    "    \"books\": [\n",
    "        \"book_id\",\n",
    "        \"title\",\n",
    "        \"stock_quantity\"\n",
    "    ],\n",
    "    \"authors\": [\n",
    "        \"author_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(join_clause(req2, SQL2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indentify_limit(tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() == \"limit\" or token.lower() == \"limits\":\n",
    "            if tokens[i+1].isdigit():\n",
    "                return f\"LIMIT {tokens[i+1]}\"\n",
    "            else:\n",
    "                return f\"LIMIT 1\"\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the top 5 most expensive products?\n",
      "['select', 'limit', '5', 'max', 'expensive', 'product']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LIMIT 5'"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q6)\n",
    "q6_nk_tok = preprocess_text(q6, constants)\n",
    "print(q6_nk_tok)\n",
    "limit_tok6 = indentify_limit(q6_nk_tok)\n",
    "limit_tok6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total number of customers?\n",
      "['select', 'count', 'customer']\n",
      "List all products names with a price greater than $50.\n",
      "['select', 'products_names', 'price', '>', '50']\n",
      "Show the first name of customers who have placed orders, sorted by their last name.\n",
      "['select', 'first_name', 'of', 'customer', 'placed', 'order', 'order', 'by', 'last_name']\n",
      "What is the total amount of orders for each customer?\n",
      "['select', 'sum', 'amount', 'of', 'order', 'group', 'by', 'custome']\n",
      "Which categories have more than 5 products?\n",
      "['select', 'category', '>', '5', 'product']\n",
      "What are the top 5 most priciest products?\n",
      "['select', 'limit', '5', 'max', 'priciest', 'product']\n",
      "Find all orders with more than 3 items.\n",
      "['select', 'all', 'order', '>', '3', 'item']\n",
      "List the top 3 customers who have spent the most on 'Electronics' category products.\n",
      "['select', 'limit', '3', 'customer', 'max', 'on', \"'Electronics'\", 'category', 'product']\n"
     ]
    }
   ],
   "source": [
    "print(q1)\n",
    "print(preprocess_text(q1, constants))\n",
    "print((q2))\n",
    "print(preprocess_text(q2, constants))\n",
    "print((q3))\n",
    "print(preprocess_text(q3, constants))\n",
    "print((q4))\n",
    "print(preprocess_text(q4, constants))\n",
    "print((q5))\n",
    "print(preprocess_text(q5, constants))\n",
    "print((q6))\n",
    "print(preprocess_text(q6, constants))\n",
    "print((q7))\n",
    "print(preprocess_text(q7, constants))\n",
    "print((q8))\n",
    "print(preprocess_text(q8, constants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "def identify_condition(tokens, req):\n",
    "    conditions = {\"where\": [], \"having\": []}\n",
    "    # \"where\" or \"having\" is in the prompt\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() == \"where\":\n",
    "            conditions['where'].append(\" \".join(tokens[i+1:])) # refine with \n",
    "            break\n",
    "\n",
    "        if token.lower() == \"having\":\n",
    "            conditions['having'].append(\" \".join(tokens[i+1:])) # refine with \n",
    "            break\n",
    "\n",
    "        \n",
    "    print(f\"Conditions: {conditions}\")\n",
    "    # Example Output: Conditions: [\"age > 30\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['categories', 'products']\n",
      "{'where': [], 'having': [{'column_or_expression': 'COUNT(*)', 'operator': '>', 'value': '5'}]}\n"
     ]
    }
   ],
   "source": [
    "def extract_conditions_with_clauses(tokens, schema):\n",
    "    \"\"\"\n",
    "    Extracts WHERE and HAVING conditions from a natural language query based on schema.\n",
    "\n",
    "    Parameters:\n",
    "    - input_query (str): The natural language query.\n",
    "    - schema (dict): A dictionary containing table names as keys and list of column names as values.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with separate lists of WHERE and HAVING conditions.\n",
    "    \"\"\"\n",
    "    # Define regex patterns for conditions and aggregate functions\n",
    "    comparison_pattern = r'(\\w+|\\w+\\s*\\(.*?\\))\\s*(=|>|<|>=|<=|!=)\\s*(\"[^\"]*\"|\\'[^\\']*\\'|\\d+(\\.\\d+)?)'\n",
    "    aggregate_functions = [\"SUM\", \"AVG\", \"COUNT\", \"MAX\", \"MIN\"]\n",
    "\n",
    "    # Find all matches in the input query\n",
    "    \n",
    "    input_query = (' ').join(tokens)\n",
    "    matches = re.findall(comparison_pattern, input_query)\n",
    "\n",
    "    where_conditions = []\n",
    "    having_conditions = []\n",
    "\n",
    "    for match in matches:\n",
    "        column_or_expression = match[0]\n",
    "        operator = match[1]\n",
    "        value = match[2]\n",
    "\n",
    "        # Remove quotes from value if they exist\n",
    "        value = value.strip('\"').strip(\"'\")\n",
    "\n",
    "        # Check if the column_or_expression contains an aggregate function\n",
    "        is_aggregate = any(func in column_or_expression.upper() for func in aggregate_functions)\n",
    "\n",
    "        # If it is an aggregate function, add it to HAVING\n",
    "        if is_aggregate or \"COUNT\" in input_query.upper():  # Handle implied aggregates\n",
    "            having_conditions.append({\n",
    "                \"column_or_expression\": column_or_expression if column_or_expression.upper() != \"COUNT\" else \"COUNT(*)\",\n",
    "                \"operator\": operator,\n",
    "                \"value\": value\n",
    "            })\n",
    "        else:\n",
    "            # Check if the column is part of the schema for WHERE clause\n",
    "            is_where = any(\n",
    "                column_or_expression in columns for columns in schema.values()\n",
    "            )\n",
    "            if is_where:\n",
    "                where_conditions.append({\n",
    "                    \"column\": column_or_expression,\n",
    "                    \"operator\": operator,\n",
    "                    \"value\": value\n",
    "                })\n",
    "            else:\n",
    "                # If not found in schema, treat it as part of HAVING by default\n",
    "                having_conditions.append({\n",
    "                    \"column_or_expression\": column_or_expression,\n",
    "                    \"operator\": operator,\n",
    "                    \"value\": value\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        \"where\": where_conditions,\n",
    "        \"having\": having_conditions\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract conditions\n",
    "q5_tok = preprocess_text(q5, constants)\n",
    "group_tok5 = identify_group_by(q5_tok, SQL)\n",
    "req = indentify_col_tables(group_tok5, SQL)\n",
    "conditions = extract_conditions_with_clauses(group_tok5, req)\n",
    "print(conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select', 'all', '>', '3', 'item']"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_tok5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indentify_order_by(tokens, req):\n",
    "    result = []\n",
    "    \n",
    "    # if \"order by\" appears in tokens\n",
    "    i = 0\n",
    "    while i < len(tokens) - 2:\n",
    "        if tokens[i].lower() == \"order\" and tokens[i+1].lower() == \"by\":\n",
    "            # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            for table, columns in req.items():\n",
    "                # for columns in req[table].values():\n",
    "                # for column in columns.keys():\n",
    "                match = get_close_matches(tokens[i+2], columns)\n",
    "                if match:\n",
    "                    if \"max\" in tokens:\n",
    "                        result.append(f\"{tokens[i]} {tokens[i+1]} {match[0]} DESC\")    \n",
    "                    else:\n",
    "                        result.append(f\"{tokens[i]} {tokens[i+1]} {match[0]}\")\n",
    "            i += 3\n",
    "        else:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    if i < len(tokens):  # Add any remaining token\n",
    "        result.extend(tokens[i:])\n",
    "\n",
    "    print(list(req.values())[0])\n",
    "    \n",
    "    # if just \"max\" appears in tokens\n",
    "    for token in result:\n",
    "        match = get_close_matches(token, list(req.values())[0])\n",
    "        if match:\n",
    "            print(match)\n",
    "            if \"max\" in result:\n",
    "                print(f\"{match[0]} desc\") # TODO: add this to template (to be created)\n",
    "            else:\n",
    "                print(f\"{match[0]}\") # TODO: add this to template (to be created)\n",
    "\n",
    "    return list(dict.fromkeys(result)) # removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the first name of customers who have placed orders, sorted by their last name.\n",
      "triggered sorted by\n",
      "['Show', 'the', 'first_name', 'of', 'customers', 'who', 'have', 'placed', 'orders', 'sorted by']\n",
      "['select', 'first_name', 'of', 'customer', 'placed', 'order', 'order', 'by', 'last_name']\n",
      "What are the top 5 most expensive products?\n",
      "['select', 'limit', '5', 'max', 'expensive', 'product']\n",
      "List the top 3 customers who have spent the most on 'Electronics' category products.\n",
      "['select', 'limit', '3', 'customer', 'max', 'on', \"'Electronics'\", 'category', 'product']\n"
     ]
    }
   ],
   "source": [
    "print((q3))\n",
    "print(preprocess_text(q3, constants))\n",
    "print((q6))\n",
    "print(preprocess_text(q6, constants))\n",
    "print((q8))\n",
    "print(preprocess_text(q8, constants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select', 'limit', '5', 'max', 'priciest', 'product']\n",
      "['products']\n",
      "['price']\n",
      "['price']\n",
      "order by price desc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['select', 'limit', '5', 'max', 'priciest', 'product']"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_token = preprocess_text(q6, constants)\n",
    "print(temp_token)\n",
    "temp_nk_token = remove_keywords(temp_token, ALL_KEYWORDS)\n",
    "group_tok = identify_group_by(temp_nk_token, SQL)\n",
    "req = indentify_col_tables(group_tok, SQL)\n",
    "indentify_order_by(temp_token, req)\n",
    "# req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triggered sorted by\n",
      "['Show', 'the', 'first_name', 'of', 'customers', 'who', 'have', 'placed', 'orders', 'sorted by']\n",
      "['customers', 'orders']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['select',\n",
       " 'first_name',\n",
       " 'of',\n",
       " 'customer',\n",
       " 'placed',\n",
       " 'order',\n",
       " 'order by last_name']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_token = preprocess_text(q3, constants)\n",
    "temp_nk_token = remove_keywords(temp_token, ALL_KEYWORDS)\n",
    "group_tok = identify_group_by(temp_nk_token, SQL)\n",
    "req = indentify_col_tables(group_tok, SQL)\n",
    "indentify_order_by(temp_token, req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6 = \"What are the top 5 most priciest products?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['price']"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_close_matches('priciest', ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "preprocess -> remove keywords -> group by -> columns & table identification -> join -> order by -> conditions -> where | having"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TEMPLATE = {\n",
    "    \"SELECT\": \"\",\n",
    "    \"FROM\": -1,\n",
    "    \"WHERE\": [], \n",
    "    'HAVING': [],\n",
    "    \"GROUP BY\": -1,\n",
    "    \"ORDER BY\": -1,\n",
    "    \"LIMIT\": -1\n",
    "}\n",
    "\n",
    "CONSTANTS = constants\n",
    "\n",
    "DB_SCHEMA = SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenerator:\n",
    "    def __init__(self):\n",
    "        self.db_schema = DB_SCHEMA\n",
    "        self.req_schema = dict()\n",
    "        self.constants = CONSTANTS\n",
    "        self.query_template = QUERY_TEMPLATE\n",
    "\n",
    "    def replace_string(self, text, replace_dict):\n",
    "        # Create a reverse mapping from values to keys\n",
    "        value_to_key = {}\n",
    "        for key, values in replace_dict.items():\n",
    "            for value in values:\n",
    "                value_to_key[value.lower()] = key.lower()\n",
    "        \n",
    "        # Create a regex pattern for matching words\n",
    "        pattern = r'\\b(' + '|'.join(re.escape(word) for word in value_to_key.keys()) + r')\\b'\n",
    "        \n",
    "        # Function to replace matched words\n",
    "        def replace_word(match):\n",
    "            return value_to_key.get(match.group(0).lower(), match.group(0))\n",
    "        \n",
    "        # Perform the replacement\n",
    "        replaced_text = re.sub(pattern, replace_word, text, flags=re.IGNORECASE)\n",
    "    \n",
    "        return replaced_text\n",
    "\n",
    "    def replace_numbers(self, token):\n",
    "        try:\n",
    "            return w2n.word_to_num(token)\n",
    "        except:\n",
    "            return token\n",
    "        \n",
    "    def merge_before_token(self, tokens, merge_key):\n",
    "\n",
    "        result = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(tokens) - 1:  # Changed to len(tokens) - 1\n",
    "            if tokens[i].lower() not in self.constants['ALL_KEYWORDS'] and (tokens[i + 1].lower() == merge_key or tokens[i + 1].lower() == f\"{merge_key}s\"):\n",
    "                print(\"condition trig\")\n",
    "                # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                i += 2\n",
    "            else:\n",
    "                result.append(tokens[i])\n",
    "                i += 1\n",
    "        if i < len(tokens):  # Add any remaining token\n",
    "            result.append(tokens[i])\n",
    "\n",
    "        print(\"merged\", result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Lowercase\n",
    "        # text = text.lower() # change it to lower while comparisson\n",
    "        \n",
    "        # Remove punctuation\n",
    "        # pattern = r'[^\\w\\s()*\\d-]|(?<!\\d)-(?!\\d)'\n",
    "        # text = re.sub(pattern, '', text)\n",
    "        text = re.sub(r'\\?', '', text)\n",
    "        text = re.sub(r'\\$', '', text)\n",
    "        text = re.sub(r'\\.$', '', text)\n",
    "        text = re.sub(r',', '', text)\n",
    "        pattern = r'\\b(\\d+)-(\\d+)\\b'\n",
    "        text = re.sub(pattern, r'\\1 to \\2', text)\n",
    "\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        # \"name\" condition\n",
    "        # result = []\n",
    "        # i = 0\n",
    "        # while i < len(tokens) - 1:  # Changed to len(tokens) - 1\n",
    "        #     if tokens[i].lower() not in self.constants['ALL_KEYWORDS'] and (tokens[i+1].lower() == \"name\" or tokens[i+1].lower() == \"names\"):\n",
    "        #         # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "        #         result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "        #         i += 2\n",
    "        #     else:\n",
    "        #         result.append(tokens[i])\n",
    "        #         i += 1\n",
    "        # if i < len(tokens):  # Add any remaining token\n",
    "        #     result.append(tokens[i])\n",
    "\n",
    "        result = self.merge_before_token(tokens, merge_key=\"name\")\n",
    "\n",
    "        result = self.merge_before_token(result, merge_key=\"date\")\n",
    "\n",
    "        i = 0\n",
    "        result2 = []\n",
    "        # group_by_keys = self.constants['sql_clauses']['GROUP BY']\n",
    "        # group_by_keys = [key.split('by')[0].strip() for key in group_by_keys if key.split('by')[0] != '']\n",
    "        # print(\"group keys\", group_by_keys)\n",
    "\n",
    "        order_by_keys = self.constants['sql_clauses']['ORDER BY']\n",
    "        order_by_keys = [key.split('by')[0].strip() for key in order_by_keys if key.split('by')[0] != '']\n",
    "        # print(\"order keys\", order_by_keys)\n",
    "\n",
    "        while i < len(result) - 1:\n",
    "            if result[i+1].lower() =='by' and result[i].lower() in order_by_keys: #(result[i].lower() in group_by_keys):\n",
    "                result2.append(f\"{result[i]} {result[i+1]}\")\n",
    "                i += 2\n",
    "            else:\n",
    "                result2.append(result[i])\n",
    "                i += 1\n",
    "        if i < len(result):  # Add any remaining token\n",
    "            result2.append(result[i])\n",
    "\n",
    "        # print(result)\n",
    "        text = ' '.join(result2)\n",
    "\n",
    "        # substitute words\n",
    "        text = self.replace_string(text, self.constants['sql_clauses'])\n",
    "        text = self.replace_string(text, self.constants['aggregate_functions'])\n",
    "        # from \\d+ -> >=\n",
    "        text = self.replace_string(text, self.constants['comparison_operators'])\n",
    "        between_pattern = r'from \\b(\\w+|\\d+)\\b to \\b(\\w+|\\d+)\\b | \\b(\\w+|\\d+)\\b to \\b(\\w+|\\d+)\\b'\n",
    "        text = re.sub(between_pattern, r\" between(\\1, \\2)\", text)\n",
    "        text = self.replace_string(text, self.constants['logical_operators'])\n",
    "\n",
    "        # Tokenization\n",
    "        # tokens = word_tokenize(text)\n",
    "        tokens = text.split()\n",
    "\n",
    "        tokens = [str(self.replace_numbers(token)) for token in tokens]\n",
    "        # print(tokens)\n",
    "\n",
    "        # Remove stopwords\n",
    "        #  load custom stop words\n",
    "        tokens = [token for token in tokens if token.lower() not in self.constants['stop_words']]\n",
    "        tokens = [token for token in tokens if token.lower() not in self.constants['stop_words']]\n",
    "        tokens = [token for token in tokens if token.lower() not in self.constants['stop_words']]\n",
    "\n",
    "        # Lemmatization (optional) // important for similarity\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return tokens# ' '.join(tokens)\n",
    "    \n",
    "    def remove_keywords(self, tokens):\n",
    "        token_without_key = [token for token in tokens if token not in self.constants['ALL_KEYWORDS']]\n",
    "\n",
    "        return token_without_key\n",
    "    \n",
    "    def identify_group_by(self, tokens):\n",
    "        result = []\n",
    "        i = 0\n",
    "        while i < len(tokens) - 2:  # Changed to len(tokens) - 1\n",
    "\n",
    "            if tokens[i].lower() == \"group\" and tokens[i+1].lower() == \"by\":\n",
    "                # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                for table, columns in self.db_schema.items():\n",
    "                    # for column in self.db_schema[table].keys():\n",
    "                    # for column in columns.keys():\n",
    "                    match = get_close_matches(tokens[i+2], columns)\n",
    "                    if match:\n",
    "                        print(tokens[i+2], \":\", match)     \n",
    "                        self.query_template[\"GROUP BY\"] = f\"{match[0]}\"                   \n",
    "                        result.append(f\"{tokens[i]} {tokens[i+1]} {match[0]}\")\n",
    "                        break\n",
    "                i += 3\n",
    "            else:\n",
    "                result.append(tokens[i])\n",
    "                i += 1\n",
    "        if i < len(tokens):  # Add any remaining token\n",
    "            result.extend(tokens[i:])\n",
    "\n",
    "        # self.query_template[\"GROUP BY\"] = -1\n",
    "        print(\"gleice: \", result)\n",
    "        return result\n",
    "    \n",
    "        # group by before this\n",
    "    def indentify_table(self, tokens):\n",
    "        \n",
    "        detected_table = [[], [], []] # [[table names], [similarities], [tokens]]\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.lower() == \"from\":\n",
    "                table_name = tokens[i + 1]\n",
    "                if table_name in self.db_schema:\n",
    "                    print(f\"Table identified: {table_name}\")\n",
    "                    return table_name\n",
    "            \n",
    "            for table in self.db_schema.keys():\n",
    "                similarity = fuzz.ratio(token, table.lower())\n",
    "                if token in detected_table[2]:\n",
    "                    # print(f\"Table identified: {token} {table} {similarity}\")\n",
    "                    # print(detected_table)\n",
    "                    i = detected_table[2].index(token)\n",
    "                    if similarity >= detected_table[1][i]:\n",
    "                        # detected_table[0] = table\n",
    "                        detected_table[1][i] = similarity\n",
    "                        detected_table[0][i] = table\n",
    "                    # print(f\"Table identified: {table} {similarity}\")\n",
    "                else:\n",
    "                    if similarity >= 70: # initial threshold\n",
    "                        detected_table[0].append(table)\n",
    "                        detected_table[1].append(similarity)\n",
    "                        detected_table[2].append(token)\n",
    "\n",
    "\n",
    "        for i in range(len(detected_table[0])):\n",
    "            if detected_table[1][i] > 85:\n",
    "                tokens.remove(detected_table[2][i])    \n",
    "            \n",
    "        # print(\"dsfsdgd\",detected_table[0])\n",
    "\n",
    "        return detected_table[0]\n",
    "\n",
    "\n",
    "    def indentify_col_tables(self, tokens):\n",
    "        res = dict()\n",
    "\n",
    "        identified_table = self.indentify_table(tokens)\n",
    "        print(\"table\",identified_table)\n",
    "        if not tokens:\n",
    "            res[identified_table[0]] = ['*']\n",
    "            return res\n",
    "\n",
    "        if identified_table:\n",
    "            for table in identified_table:\n",
    "                res[table] = dict()\n",
    "                for token in tokens:\n",
    "                    for column in self.db_schema[table].keys():\n",
    "                        # for column in columns.keys():\n",
    "                        if column=='pk' or column=='fk':\n",
    "                            continue\n",
    "                        else:\n",
    "                            similarity = fuzz.ratio(token, column.lower())\n",
    "                            if similarity >= 55:\n",
    "                                if res[table].get(token):\n",
    "                                    # print(\"case 1\")\n",
    "                                    # print(token, \":\", column, similarity)\n",
    "                                    old_similarity = list(res[table][token].values())[0]\n",
    "                                    # print(\"sim\", old_similarity)\n",
    "                                    if similarity > old_similarity:\n",
    "                                        res[table][token] = {column: similarity}\n",
    "                                else:\n",
    "                                    # print(\"case 2\")\n",
    "                                    # print(token, \":\", column, similarity)\n",
    "                                    res[table][token] = {column: similarity}\n",
    "                                # print(res)\n",
    "                                # if res.get(identified_table):\n",
    "                                #     res[identified_table].append(column)\n",
    "                                # else:\n",
    "                                #     res[identified_table] = [column]\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                for table, columns in self.db_schema.items():\n",
    "                    # for column in self.db_schema[table].keys():\n",
    "                    # # for column in columns.keys():\n",
    "                    #     if column=='pk' or column=='fk':\n",
    "                    #         continue\n",
    "                    #     else:\n",
    "                    \n",
    "                    cols = list(map(str.lower, list(columns.keys())))\n",
    "                            \n",
    "                    match = get_close_matches(token, cols)\n",
    "                    # print(\"abracadabra: \", token, columns)\n",
    "                    if match:\n",
    "                        # print(\"match found:\",match)\n",
    "                        # print(\"*\"*4, token, \":\", match[0])\n",
    "                        # if token in columns: # replace by fuzzy logic\n",
    "                        # print(f\"Column identified: {column} in table {table}\")\n",
    "                        if res.get(table):\n",
    "                            res[table].add(match[0])\n",
    "                        else:\n",
    "                            res[table] = set(match)\n",
    "\n",
    "        if identified_table:\n",
    "            result = {}\n",
    "            for table, token in res.items():\n",
    "                cols = set()\n",
    "                for col in token.values():\n",
    "                    cols.update(col.keys())\n",
    "                result[table] = cols\n",
    "\n",
    "            result = {key: value for key, value in result.items() if value != set()}\n",
    "            return result\n",
    "        \n",
    "\n",
    "        res = {key: value for key, value in res.items() if value != set()}    \n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    # JOIN\n",
    "    def create_graph(self, directional=False):\n",
    "        if directional:\n",
    "            G = nx.DiGraph()\n",
    "        else:\n",
    "            G = nx.Graph()\n",
    "\n",
    "        # Add nodes and edges based on the SQL dictionary\n",
    "        for table, details in self.db_schema.items():\n",
    "            G.add_node(table)  # Add table as a node\n",
    "            for fk_table in details['fk']:  # Iterate through foreign keys\n",
    "                G.add_edge(table, fk_table)  # Create an edge from current table to foreign key table\n",
    "\n",
    "        # Get nodes and edges for verification\n",
    "        nodes = list(G.nodes)\n",
    "        edges = list(G.edges)\n",
    "\n",
    "        # Output nodes and edges\n",
    "        # print(nodes)\n",
    "        # print(edges)\n",
    "\n",
    "        return G\n",
    "    \n",
    "    def required_tables_graph(self, G, start, end, required_tables):\n",
    "        def dijkstra(graph, start, end):\n",
    "            distances = {node: float('infinity') for node in graph}\n",
    "            distances[start] = 0\n",
    "            pq = [(0, start)]\n",
    "            previous = {node: None for node in graph}\n",
    "\n",
    "            while pq:\n",
    "                current_distance, current_node = heapq.heappop(pq)\n",
    "\n",
    "                if current_node == end:\n",
    "                    path = []\n",
    "                    while current_node:\n",
    "                        path.append(current_node)\n",
    "                        current_node = previous[current_node]\n",
    "                    return path[::-1], current_distance\n",
    "\n",
    "                for neighbor in graph[current_node]:\n",
    "                    distance = current_distance + 1  # All edges have weight of 1\n",
    "                    if distance < distances[neighbor]:\n",
    "                        distances[neighbor] = distance\n",
    "                        previous[neighbor] = current_node\n",
    "                        heapq.heappush(pq, (distance, neighbor))\n",
    "\n",
    "            return None, float('infinity')\n",
    "\n",
    "        required_tables = set(required_tables) - {start, end}\n",
    "        best_path = None\n",
    "        best_distance = float('infinity')\n",
    "\n",
    "        def dfs(current_path, current_distance, remaining_required):\n",
    "            nonlocal best_path, best_distance\n",
    "\n",
    "            if not remaining_required:\n",
    "                path, distance = dijkstra(G, current_path[-1], end)\n",
    "                if path:\n",
    "                    total_path = current_path + path[1:]\n",
    "                    total_distance = current_distance + distance\n",
    "                    if total_distance < best_distance:\n",
    "                        best_path = total_path\n",
    "                        best_distance = total_distance\n",
    "                return\n",
    "\n",
    "            for node in remaining_required:\n",
    "                path, distance = dijkstra(G, current_path[-1], node)\n",
    "                if path:\n",
    "                    new_path = current_path + path[1:]\n",
    "                    new_distance = current_distance + distance\n",
    "                    new_remaining = remaining_required - {node}\n",
    "                    dfs(new_path, new_distance, new_remaining)\n",
    "\n",
    "        dfs([start], 0, required_tables)\n",
    "\n",
    "        return best_path, best_distance\n",
    "    \n",
    "    def graph_sort(self, edges):\n",
    "        counts = Counter(t[0] for t in edges)\n",
    "        \n",
    "        # Sort tuples based on the counts in descending order\n",
    "        sorted_tuples = sorted(edges, key=lambda x: counts[x[0]], reverse=True)\n",
    "        \n",
    "        return sorted_tuples\n",
    "\n",
    "    def remove_duplicate_keys(self, edges):\n",
    "\n",
    "        filtered_result = dict()\n",
    "\n",
    "        for edge in edges:\n",
    "\n",
    "            # Extract the keys from the dictionaries\n",
    "            table0 = self.req_schema[edge[0]]\n",
    "            table1 = self.req_schema[edge[1]]\n",
    "\n",
    "            # Create a new dictionary with only the edge[0] key\n",
    "            # and its value as the set of keys that are unique to edge[0]\n",
    "            filtered_result[edge[0]]= table0 - table1\n",
    "            if table1 - table0:\n",
    "                filtered_result[edge[1]]= table1 - table0\n",
    "\n",
    "\n",
    "            # If the result is an empty set, we want to keep the original edge[0] value\n",
    "            if not filtered_result[edge[0]]:\n",
    "                filtered_result[edge[0]] = self.req_schema[edge[0]]\n",
    "\n",
    "        print(\"updated schema:\",filtered_result)\n",
    "\n",
    "        # Get all unique keys across all tables\n",
    "        # all_keys = set().union(*self.req_schema.values())\n",
    "        \n",
    "        # # Create a dictionary to store the count of each key\n",
    "        # key_count = {key: sum(1 for table_keys in self.req_schema.values() if key in table_keys) for key in all_keys}\n",
    "        \n",
    "        # # Create the result dictionary\n",
    "        # result = {}\n",
    "        \n",
    "        # for table, keys in self.req_schema.items():\n",
    "        #     # Keep only the keys that are unique to this table\n",
    "        #     unique_keys = {key for key in keys if key_count[key] == 1}\n",
    "            \n",
    "        #     # If there are no unique keys, keep all original keys for this table\n",
    "        #     if not unique_keys:\n",
    "        #         unique_keys = keys\n",
    "            \n",
    "        #     # Add the table and its unique keys to the result\n",
    "        #     result[table] = unique_keys\n",
    "        \n",
    "        return filtered_result\n",
    "\n",
    "    def join_clause(self):\n",
    "        clause = \"\"\n",
    "\n",
    "        # not required\n",
    "        # max_cols = 0\n",
    "        # prim_table = None\n",
    "        # # change logic based on foreign key numbers\n",
    "        # for table, cols in req_schema.items():\n",
    "        #     if len(cols) > max_cols:\n",
    "        #         max_cols = len(cols)\n",
    "        #         prim_table = table\n",
    "        \n",
    "        # # pk_col = db_schema[prim_table]['pk']\n",
    "        # print(prim_table)\n",
    "\n",
    "        # create graph from Database schema\n",
    "        db_graph = self.create_graph()\n",
    "        db_dir_graph = self.create_graph(directional=True)\n",
    "\n",
    "        # print(db_dir_graph)\n",
    "\n",
    "        required_tables = list(self.req_schema.keys())\n",
    "\n",
    "        print(required_tables)\n",
    "\n",
    "        min_dist = float('inf')\n",
    "        for st_table in required_tables:\n",
    "            for end_table in required_tables:\n",
    "                sub_graph, distance = self.required_tables_graph(db_graph, st_table, end_table, required_tables)\n",
    "                if distance < min_dist:\n",
    "                    join_graph = OrderedSet(sub_graph)\n",
    "                    min_dist = distance\n",
    "\n",
    "        print(join_graph)\n",
    "\n",
    "\n",
    "        req_graph = []\n",
    "\n",
    "        for edge in db_dir_graph.edges:\n",
    "            # assumption binary relations between tables\n",
    "            if edge[0] in join_graph and edge[1] in join_graph:\n",
    "                req_graph.append(edge)\n",
    "\n",
    "        req_graph = self.graph_sort(req_graph)\n",
    "        print(\"required graph: \", req_graph)\n",
    "\n",
    "        self.req_schema = self.remove_duplicate_keys(req_graph)\n",
    "\n",
    "        if len(self.req_schema) >1 :\n",
    "\n",
    "            clause += f\"{req_graph[0][0]}\\n\"\n",
    "\n",
    "            for i, edge in enumerate(req_graph):\n",
    "                table1 = edge[0]\n",
    "                table2 = edge[1]\n",
    "                # print(edge)\n",
    "                fk_col = self.db_schema[table1]['fk'][table2] # foreign key corresponding to 2nd table  primary key\n",
    "                pk_col = self.db_schema[table2]['pk'][0] # primary key of 2nd table\n",
    "                clause += f\"JOIN {table2} ON {table1}.{fk_col}={table2}.{pk_col} \\n\"\n",
    "\n",
    "            self.query_template['FROM'] = clause\n",
    "        else:\n",
    "            clause += f\"{req_graph[0][0]}\\n\"\n",
    "            self.query_template['FROM'] = clause\n",
    "\n",
    "            \n",
    "        return clause\n",
    "\n",
    "    def aggregate_parser(self, tokens):\n",
    "\n",
    "        res = []\n",
    "        i = 0\n",
    "        thresh = 50\n",
    "        pot_col = \"\"\n",
    "        while i< len(tokens):\n",
    "            if tokens[i].upper() in list(self.constants['aggregate_functions'].keys()):\n",
    "                for table in self.db_schema.keys():\n",
    "                    print(table)\n",
    "                    cols = list(self.db_schema[table].keys()) # too check with missing cols from prev functions\n",
    "                    print(cols)\n",
    "                    for col in cols:\n",
    "                        similarity = fuzz.ratio(tokens[i+1], col)\n",
    "                        if similarity>thresh:\n",
    "                            thresh = similarity\n",
    "                            pot_col = f\"{table.lower()}.{col.lower()}\"\n",
    "                            print(thresh, pot_col)\n",
    "                res.append(f\"{tokens[i].upper()}({pot_col})\")\n",
    "                i+=2\n",
    "                    # i+=1\n",
    "            else:\n",
    "                res.append(tokens[i])\n",
    "                i+=1\n",
    "            # i+=1\n",
    "            \n",
    "        print(\"agg_parser\",res)\n",
    "\n",
    "        return res\n",
    "\n",
    "    # CONDITION - WHERE & HAVING    \n",
    "    def extract_conditions(self, tokens):\n",
    "        \"\"\"\n",
    "        Extracts WHERE and HAVING conditions from a natural language query based on keywords 'where' and 'having',\n",
    "        or based on comparison operators (=, >, <, <=, >=).\n",
    "\n",
    "        Parameters:\n",
    "        - input_query (str): The natural language query.\n",
    "        - req_schema (dict): A dictionary containing table names as keys and list of column names as values.\n",
    "\n",
    "        Returns:\n",
    "        - dict: A dictionary with separate lists of WHERE and HAVING conditions, and aggregate functions.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens_copy = tokens.copy()\n",
    "        print(tokens_copy)\n",
    "        # Define regex patterns for comparison operators and aggregate functions\n",
    "        comparison_pattern = r'(=|>|<|>=|<=|!=)'  # To locate comparison operators\n",
    "        # aggregate_functions = [\"SUM\", \"AVG\", \"COUNT\", \"MAX\", \"MIN\"]\n",
    "\n",
    "        # Initialize results\n",
    "        where_conditions = []\n",
    "        having_conditions = []\n",
    "\n",
    "        # Split input query into tokens for easier parsing\n",
    "        # tokens = input_query.split()\n",
    "        input_query = (' ').join(tokens_copy)\n",
    "        print(input_query)\n",
    "\n",
    "        # Detect keywords and their positions\n",
    "        where_pos = input_query.lower().find(\"where\")\n",
    "        having_pos = input_query.lower().find(\"having\")\n",
    "\n",
    "        # Helper function to find nearest column and value around a comparison operator\n",
    "        def find_column_and_value(tokens_copy, operator_index, req_schema):\n",
    "            column = None\n",
    "            value = None\n",
    "\n",
    "            # Look before and after the operator for a column and a value\n",
    "            if operator_index > 0:\n",
    "                potential_column = tokens_copy[operator_index - 1]\n",
    "                formatted_keys = [f'{column.lower()}' \n",
    "                        for table, columns in req_schema.items() \n",
    "                        for column in columns]\n",
    "                print(req_schema)\n",
    "                print(\"inside_col_cal_func\", potential_column,formatted_keys)\n",
    "                match = get_close_matches(potential_column.lower(), formatted_keys)\n",
    "                print(match)\n",
    "                if match:\n",
    "                    column = potential_column\n",
    "\n",
    "            if operator_index < len(tokens_copy) - 1:\n",
    "                potential_value = tokens_copy[operator_index + 1]\n",
    "                # Check if it's a quoted value or a number\n",
    "                if re.match(r\"^'[^']*'$|^\\d+(\\.\\d+)?$|^(?:\\d{2}[/-]\\d{2}[/-]\\d{4})$\", potential_value):\n",
    "                    value = potential_value.strip(\"'\").strip('\"')\n",
    "                # print(\"%%%%%\",value)\n",
    "\n",
    "            return column, value\n",
    "\n",
    "        res = self.aggregate_parser(tokens_copy)\n",
    "\n",
    "        agg_flag = False\n",
    "\n",
    "        for token in res:\n",
    "            for agg in aggregate_functions.keys():\n",
    "                if agg.lower() in token.lower():\n",
    "                    agg_flag = True\n",
    "\n",
    "        for i, token in enumerate(res):\n",
    "            if re.match(comparison_pattern, token):  # If it's a comparison operator\n",
    "                column, value = find_column_and_value(res, i, self.req_schema)\n",
    "                print(\"col and val\", column, value)\n",
    "\n",
    "                if column and value:\n",
    "                    condition = {\n",
    "                        \"column\": column,\n",
    "                        \"operator\": token,\n",
    "                        \"value\": value\n",
    "                    }\n",
    "                    print(\"cond:\", condition)\n",
    "                    print(\"where:\", where_pos)\n",
    "                    print(\"having:\", having_pos)\n",
    "\n",
    "                    # Check if the condition belongs to WHERE or HAVING based on position\n",
    "                    if where_pos != -1 and having_pos == -1: # iff \"where\" in tokens\n",
    "                        where_conditions.append(condition)\n",
    "                    elif having_pos != -1 and where_pos == -1: # iff \"where\" in tokens\n",
    "                        having_conditions.append(condition)\n",
    "                    elif agg_flag: # iff \"aggregate_function\" in tokens\n",
    "                        having_conditions.append(condition)\n",
    "                    else: # default\n",
    "                        where_conditions.append(condition)\n",
    "\n",
    "        # # Detect aggregate functions\n",
    "        # for func in aggregate_functions:\n",
    "        #     match = re.search(rf'\\b{func}\\s*\\((.*?)\\)', input_query, re.IGNORECASE)\n",
    "        #     if match:\n",
    "        #         aggregate_query = {\n",
    "        #             \"function\": func.upper(),\n",
    "        #             \"column_or_expression\": match.group(1).strip() or \"*\"\n",
    "        #         }\n",
    "        #         break  # Only detect the first aggregate function\n",
    "\n",
    "        self.query_template['WHERE'] = where_conditions\n",
    "        self.query_template['HAVING'] = having_conditions\n",
    "\n",
    "        return res, {\"where\": where_conditions, \"having\": having_conditions}\n",
    "\n",
    "    def indentify_order_by(self, tokens):\n",
    "        result = []\n",
    "        \n",
    "        # if \"order by\" appears in tokens\n",
    "        i = 0\n",
    "        while i < len(tokens) - 2:\n",
    "            if tokens[i].lower() == \"order\" and tokens[i+1].lower() == \"by\":\n",
    "                # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                for table, columns in self.req_schema.items():\n",
    "                    # for columns in self.req_schema[table].values():\n",
    "                    # for column in columns.keys():\n",
    "                    match = get_close_matches(tokens[i+2], columns)\n",
    "                    if match:\n",
    "                        if \"max\" in tokens:\n",
    "                            self.query_template[\"ORDER BY\"] = f\"{table}.{match[0]} DESC\"\n",
    "                            result.append(f\"{tokens[i]} {tokens[i+1]} {table}.{match[0]} DESC\")    \n",
    "                        else:\n",
    "                            self.query_template[\"ORDER BY\"] = f\"{table}.{match[0]}\"\n",
    "                            result.append(f\"{tokens[i]} {tokens[i+1]} {table}.{match[0]}\")\n",
    "                i += 3\n",
    "            else:\n",
    "                result.append(tokens[i])\n",
    "                i += 1\n",
    "        if i < len(tokens):  # Add any remaining token\n",
    "            result.extend(tokens[i:])\n",
    "\n",
    "        print(self.req_schema)\n",
    "\n",
    "        # print(list(self.req_schema.values())[0])\n",
    "        \n",
    "        # if just \"max\" appears in tokens\n",
    "        for token in result:\n",
    "            for table, cols in self.req_schema.items():\n",
    "                match = get_close_matches(token, cols)\n",
    "                if match:\n",
    "                    print(match)\n",
    "                    if \"max\" in result:\n",
    "                        self.query_template[\"ORDER BY\"] = f\"{table}.{match[0]} DESC\"\n",
    "                        print(f\"{match[0]} desc\") \n",
    "                    else:\n",
    "                        self.query_template[\"ORDER BY\"] = f\"{table}.{match[0]}\"\n",
    "                        print(f\"{match[0]}\")\n",
    "\n",
    "        return list(dict.fromkeys(result)) # removing duplicates\n",
    "    \n",
    "    # LIMIT\n",
    "    def indentify_limit(self, tokens):\n",
    "        limit = -1\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.lower() == \"limit\" or token.lower() == \"limits\":\n",
    "                if tokens[i+1].isdigit():\n",
    "                    limit = tokens[i+1]\n",
    "                    self.query_template[\"LIMIT\"] = tokens[i+1]\n",
    "                    return\n",
    "                else:\n",
    "                    limit = 1\n",
    "                    self.query_template[\"LIMIT\"] = 1\n",
    "        \n",
    "        # self.query_template[\"LIMIT\"] = -1\n",
    "\n",
    "        return limit\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def select_validator(self, tokens):\n",
    "        # making sure all the required columns are present in select clause\n",
    "        \n",
    "        # check group by and having condition\n",
    "        self.query_template['SELECT'] = \"\"\n",
    "        select_cols = set()\n",
    "\n",
    "        agg_funcs = list(aggregate_functions.keys())\n",
    "\n",
    "        for token in tokens:\n",
    "            for agg in aggregate_functions:\n",
    "                if agg in token:\n",
    "                    select_cols.add(token)\n",
    "\n",
    "        for table, cols in self.req_schema.items():\n",
    "            for col in cols:\n",
    "                select_cols.add(f\"{table}.{col}\")\n",
    "\n",
    "        group_by_col = self.query_template['GROUP BY']\n",
    "        if group_by_col:\n",
    "            for table, cols in self.req_schema.items():\n",
    "                if group_by_col in cols:\n",
    "                    select_cols.add(f\"{table}.{group_by_col}\")\n",
    "                    self.query_template['GROUP BY'] = f\"{table}.{group_by_col}\"\n",
    "\n",
    "        \n",
    "        having_col = self.query_template['HAVING']\n",
    "        if group_by_col and having_col:\n",
    "            for table, cols in self.req_schema.items():\n",
    "                if group_by_col in cols:\n",
    "                    select_cols.add(f\"{table}.{group_by_col}\")\n",
    "        \n",
    "\n",
    "        for cols in select_cols:\n",
    "            self.query_template['SELECT']+=f\"{cols}, \"\n",
    "\n",
    "        self.query_template['SELECT'] = self.query_template['SELECT'].strip(', ')\n",
    "\n",
    "        return\n",
    "\n",
    "    # GRAPH GENERATOR - seperate function/class\n",
    "    # def graph_generator():\n",
    "    #     pass\n",
    "\n",
    "    # template generator - not required\n",
    "    def query_template_generator(self, input) -> dict:\n",
    "\n",
    "        # preprocess -> remove keywords -> group by -> columns & table identification \n",
    "        # -> join -> order by -> conditions -> where | having\n",
    "\n",
    "        self.query_template = QUERY_TEMPLATE.copy()\n",
    "\n",
    "        self.input = input\n",
    "        print(\"******input\",self.input)\n",
    "\n",
    "        self.tokens = self.preprocess_text(input)\n",
    "        print(\"******tokens\",self.tokens)\n",
    "        self.tokens_nk = self.remove_keywords(self.tokens)\n",
    "        print(\"******tokens_nk\",self.tokens_nk)\n",
    "\n",
    "        self.tokens_group = self.identify_group_by(self.tokens_nk)\n",
    "        print(\"******tokens_group\",self.tokens_group)\n",
    "        self.req_schema = self.indentify_col_tables(self.tokens_group)\n",
    "        print(\"******req_schema\",self.req_schema)\n",
    "\n",
    "        if len(self.req_schema)>1:\n",
    "            self.from_clause = self.join_clause()\n",
    "            print(\"******\",self.from_clause)\n",
    "        else:\n",
    "            for table in self.req_schema.keys():\n",
    "                self.query_template['FROM'] = table\n",
    "\n",
    "        self.agg_tokens, self.conditions = self.extract_conditions(self.tokens)\n",
    "        print(\"******conditions\",self.conditions)\n",
    "        print(\"******agg_tokens\",self.agg_tokens)\n",
    "\n",
    "\n",
    "        self.order_by = self.indentify_order_by(self.tokens)\n",
    "        print(\"******order_by\",self.order_by)\n",
    "\n",
    "        self.limit = self.indentify_limit(self.tokens)\n",
    "        print(\"******limit\",self.limit)\n",
    "\n",
    "        self.select = self.select_validator(self.agg_tokens)\n",
    "\n",
    "\n",
    "        return self.query_template\n",
    "\n",
    "    # Driver function - Query generator\n",
    "    # def sql_parser(self, parsed_dict: dict):\n",
    "    #     query = \"\"\n",
    "\n",
    "    #     return query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******input Find the patients names, admissions roomnumber  and the insurance provider\n",
      "condition trig\n",
      "merged ['Find', 'the', 'patients_names', 'admissions', 'roomnumber', 'and', 'the', 'insurance', 'provider']\n",
      "merged ['Find', 'the', 'patients_names', 'admissions', 'roomnumber', 'and', 'the', 'insurance', 'provider']\n",
      "******tokens ['select', 'patients_names', 'admission', 'roomnumber', 'and', 'insurance', 'provider']\n",
      "******tokens_nk ['patients_names', 'admission', 'roomnumber', 'insurance', 'provider']\n",
      "gleice:  ['patients_names', 'admission', 'roomnumber', 'insurance', 'provider']\n",
      "******tokens_group ['patients_names', 'admission', 'roomnumber', 'insurance', 'provider']\n",
      "table ['patients', 'admissions', 'insurance']\n",
      "******req_schema {'patients': {'patientname'}, 'admissions': {'patientid', 'roomnumber'}, 'insurance': {'insuranceprovider', 'patientid'}}\n",
      "['patients', 'admissions', 'insurance']\n",
      "OrderedSet(['patients', 'insurance', 'admissions'])\n",
      "required graph:  [('admissions', 'patients'), ('admissions', 'insurance'), ('insurance', 'patients')]\n",
      "updated schema: {'admissions': {'roomnumber'}, 'patients': {'patientname'}, 'insurance': {'insuranceprovider', 'patientid'}}\n",
      "****** admissions\n",
      "JOIN patients ON admissions.patientid=patients.patientid \n",
      "JOIN insurance ON admissions.insuranceid=insurance.insuranceid \n",
      "JOIN patients ON insurance.patientid=patients.patientid \n",
      "\n",
      "['select', 'patients_names', 'admission', 'roomnumber', 'and', 'insurance', 'provider']\n",
      "select patients_names admission roomnumber and insurance provider\n",
      "agg_parser ['select', 'patients_names', 'admission', 'roomnumber', 'and', 'insurance', 'provider']\n",
      "******conditions {'where': [], 'having': []}\n",
      "******agg_tokens ['select', 'patients_names', 'admission', 'roomnumber', 'and', 'insurance', 'provider']\n",
      "{'admissions': {'roomnumber'}, 'patients': {'patientname'}, 'insurance': {'insuranceprovider', 'patientid'}}\n",
      "['patientname']\n",
      "patientname\n",
      "['patientid']\n",
      "patientid\n",
      "['roomnumber']\n",
      "roomnumber\n",
      "['insuranceprovider']\n",
      "insuranceprovider\n",
      "['insuranceprovider']\n",
      "insuranceprovider\n",
      "******order_by ['select', 'patients_names', 'admission', 'roomnumber', 'and', 'insurance', 'provider']\n",
      "******limit -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SELECT': 'insurance.patientid, admissions.roomnumber, insurance.insuranceprovider, patients.patientname',\n",
       " 'FROM': 'admissions\\nJOIN patients ON admissions.patientid=patients.patientid \\nJOIN insurance ON admissions.insuranceid=insurance.insuranceid \\nJOIN patients ON insurance.patientid=patients.patientid \\n',\n",
       " 'WHERE': [],\n",
       " 'HAVING': [],\n",
       " 'GROUP BY': -1,\n",
       " 'ORDER BY': 'insurance.insuranceprovider',\n",
       " 'LIMIT': -1}"
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qg = QueryGenerator()\n",
    "\n",
    "q = \"Find the patients names, admissions roomnumber  and the insurance provider\"\n",
    "\n",
    "qg.query_template_generator(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 987,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_key = \"name\"\n",
    "\n",
    "\"name\" == merge_key or \"name\" == f\"{merge_key}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_date']"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_close_matches(\"order\", [\"order_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLGenerator(QueryGenerator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def sql_parser(self, input):\n",
    "        query_template = super().query_template_generator(self, input)\n",
    "\n",
    "        query = \"\"\n",
    "\n",
    "        for key, values in query_template.items():\n",
    "            if query_template[key] != -1 or query_template[key]:\n",
    "                if key!=\"WHERE\" and key != \"HAVING\":\n",
    "                    print(key)\n",
    "                    query += f\"{key.upper()} {values}\\n\"\n",
    "                    # print(query)\n",
    "                else:\n",
    "\n",
    "                    print(\"cond\")\n",
    "                    for cond in query_template[key]:\n",
    "                        print((\" \").join(cond.values()))\n",
    "                        query += f\"{key} {(\" \").join(cond.values())} AND \"\n",
    "                    \n",
    "                    query = query[:-3] # removing extra AND\n",
    "                    query +=  \"\\n\"\n",
    "\n",
    "        query = query.strip()\n",
    "        query += \";\"\n",
    "\n",
    "        return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "cond\n",
      "quantity > 30\n",
      "cond\n",
      "GROUP BY\n",
      "ORDER BY\n",
      "LIMIT\n",
      "SELECT products.stock_quantity, order_items.quantity\n",
      "FROM order_items\n",
      "JOIN products ON order_items.product_id=products.product_id \n",
      "WHERE quantity > 30\n",
      "GROUP BY -1\n",
      "ORDER BY products.stock_quantity DESC\n",
      "LIMIT 3;\n"
     ]
    }
   ],
   "source": [
    "check_dict = {'SELECT': 'products.stock_quantity, order_items.quantity',\n",
    " 'FROM': 'order_items\\nJOIN products ON order_items.product_id=products.product_id \\n',\n",
    " 'WHERE': [{'column': 'quantity', 'operator': '>', 'value': '30'}],\n",
    " 'HAVING': [],\n",
    " 'GROUP BY': -1,\n",
    " 'ORDER BY': 'products.stock_quantity DESC',\n",
    " 'LIMIT': 3}\n",
    "\n",
    "query = \"\"\n",
    "\n",
    "for key, values in check_dict.items():\n",
    "    if check_dict[key] != -1 or check_dict[key]:\n",
    "        if key==\"FROM\":\n",
    "            query += f\"{key.upper()} {values}\"\n",
    "        elif key!=\"WHERE\" and key != \"HAVING\":\n",
    "            print(key)\n",
    "            query += f\"{key.upper()} {values}\\n\"\n",
    "            # print(query)\n",
    "        else:\n",
    "\n",
    "            print(\"cond\")\n",
    "            for cond in check_dict[key]:\n",
    "                print((\" \").join(cond.values()))\n",
    "                query += f\"{key} {(\" \").join(cond.values())} AND \"\n",
    "            \n",
    "            query = query[:-3] # removing extra AND\n",
    "            query +=  \"\\n\"\n",
    "\n",
    "query = query.strip()\n",
    "query += \";\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoSQLGenerator(QueryGenerator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "    def mongod_parser(self, input):\n",
    "        query_template = super().query_template_generator(self, input=input)\n",
    "\n",
    "        mongodb_query = \"db\"\n",
    "        prim_table = query_template['FROM'].split()[0]\n",
    "\n",
    "        conditions\n",
    "\n",
    "        if query_template['GROUP BY']==-1:\n",
    "            mongodb_query += f\".{prim_table}\"\n",
    "            if \"DISTINCT\" in query_template['SELECT']:\n",
    "                monogodb_query += f\"distinct\"\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'match': {'quantity': {'$gt': 30}}, 'group': {'_id': {'products.stock_quantity': '$products.stock_quantity'}, 'order_items.quantity': {'$first': '$order_items.quantity'}}, 'project': {'products.stock_quantity': 1, 'order_items.quantity': 1}, 'sort': {'products.stock_quantity': -1}, 'limit': 3}\n"
     ]
    }
   ],
   "source": [
    "def sql_to_mongodb_query(sql_dict):\n",
    "    \"\"\"\n",
    "    Converts a SQL-like dictionary to a MongoDB aggregation pipeline in dictionary format.\n",
    "\n",
    "    Args:\n",
    "        sql_dict (dict): SQL-like query dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: MongoDB aggregation pipeline in dictionary format.\n",
    "    \"\"\"\n",
    "    # Initialize MongoDB query components\n",
    "    match = {}\n",
    "    project = {}\n",
    "    sort = {}\n",
    "    group = None\n",
    "    limit = None\n",
    "\n",
    "    # Process WHERE clause into $match\n",
    "    where_clauses = sql_dict.get('WHERE', [])\n",
    "    for condition in where_clauses:\n",
    "        column = condition['column']\n",
    "        operator = condition['operator']\n",
    "        value = condition['value']\n",
    "\n",
    "        # Map SQL operators to MongoDB operators\n",
    "        operator_map = {\n",
    "            '=': '$eq',\n",
    "            '>': '$gt',\n",
    "            '<': '$lt',\n",
    "            '>=': '$gte',\n",
    "            '<=': '$lte',\n",
    "            '<>': '$ne'\n",
    "        }\n",
    "\n",
    "        mongo_operator = operator_map.get(operator)\n",
    "        if mongo_operator:\n",
    "            match[column] = {mongo_operator: int(value) if value.isdigit() else value}\n",
    "\n",
    "    # Handle SELECT clause as $project\n",
    "    select_columns = sql_dict.get('SELECT', '').split(', ')\n",
    "    for column in select_columns:\n",
    "        project[column.strip()] = 1\n",
    "\n",
    "    # Process GROUP BY into $group\n",
    "    group_by = sql_dict.get('GROUP BY')\n",
    "    if group_by != -1:  # Check if GROUP BY is specified\n",
    "        group = {\"_id\": {col.strip(): f\"${col.strip()}\" for col in group_by.split(', ') if col.strip() != ''}}\n",
    "        # Add SELECT columns to $group for aggregation\n",
    "        for column in select_columns:\n",
    "            column = column.strip()\n",
    "            if column not in group[\"_id\"]:\n",
    "                group[column] = {\"$first\": f\"${column}\"}  # Default to $first for other fields\n",
    "\n",
    "    # Process ORDER BY into $sort\n",
    "    order_by = sql_dict.get('ORDER BY', '')\n",
    "    if order_by:\n",
    "        column, direction = order_by.split(' ')\n",
    "        sort[column.strip()] = -1 if direction.strip().upper() == 'DESC' else 1\n",
    "\n",
    "    # Handle LIMIT\n",
    "    limit = sql_dict.get('LIMIT')\n",
    "\n",
    "    # Construct MongoDB query dictionary\n",
    "    query_dict = {}\n",
    "    if match:\n",
    "        query_dict['match'] = match\n",
    "    if group:\n",
    "        query_dict['group'] = group\n",
    "    if project:\n",
    "        query_dict['project'] = project\n",
    "    if sort:\n",
    "        query_dict['sort'] = sort\n",
    "    if limit:\n",
    "        query_dict['limit'] = int(limit)\n",
    "\n",
    "    return query_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sql_dict = {\n",
    "    'SELECT': 'products.stock_quantity, order_items.quantity',\n",
    "    'FROM': 'order_items\\nJOIN products ON order_items.product_id=products.product_id \\n',\n",
    "    'WHERE': [{'column': 'quantity', 'operator': '>', 'value': '30'}],\n",
    "    'HAVING': [],\n",
    "    'GROUP BY': 'products.stock_quantity',\n",
    "    'ORDER BY': 'products.stock_quantity DESC',\n",
    "    'LIMIT': 3\n",
    "}\n",
    "\n",
    "mongodb_query = sql_to_mongodb_query(sql_dict)\n",
    "print(mongodb_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
