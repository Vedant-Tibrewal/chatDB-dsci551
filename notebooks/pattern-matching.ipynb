{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "from thefuzz import fuzz\n",
    "import networkx as nx\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "from word2number import w2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vedanttibrewal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vedanttibrewal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vedanttibrewal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = {\n",
    "    \"products\": {\n",
    "        \"pk\": [\"product_id\"],\n",
    "        \"fk\": {\"categories\": \"category_id\"},\n",
    "        \"product_id\": \"INT\",\n",
    "        \"product_name\": \"VARCHAR(255)\",\n",
    "        \"description\": \"TEXT\",\n",
    "        \"price\": \"DECIMAL(10, 2)\",\n",
    "        \"stock_quantity\": \"INT\",\n",
    "        \"category_id\": \"INT\"\n",
    "    },\n",
    "    \"categories\": {\n",
    "        \"pk\": [\"category_id\"],\n",
    "        \"fk\": {},\n",
    "        \"category_id\": \"INT\",\n",
    "        \"category_name\": \"VARCHAR(100)\",\n",
    "        \"description\": \"TEXT\"\n",
    "    },\n",
    "    \"customers\": {\n",
    "        \"pk\": [\"customer_id\"],\n",
    "        \"fk\": {},\n",
    "        \"customer_id\": \"INT\",\n",
    "        \"first_name\": \"VARCHAR(50)\",\n",
    "        \"last_name\": \"VARCHAR(50)\",\n",
    "        \"email\": \"VARCHAR(100)\",\n",
    "        \"phone_number\": \"VARCHAR(20)\",\n",
    "        \"address\": \"TEXT\"\n",
    "    },\n",
    "    \"orders\": {\n",
    "        \"pk\": [\"order_id\"],\n",
    "        \"fk\": {\n",
    "            \"customers\": \"customer_id\"\n",
    "        },\n",
    "        \"order_id\": \"INT\",\n",
    "        \"customer_id\": \"INT\",\n",
    "        \"order_date\": \"DATE\",\n",
    "        \"total_amount\": \"DECIMAL(10, 2)\",\n",
    "        \"status\": \"VARCHAR(20)\"\n",
    "    },\n",
    "    \"order_items\": {\n",
    "        \"pk\": [\"order_item_id\"],\n",
    "        \"fk\": {\n",
    "            \"orders\": \"order_id\",\n",
    "            \"products\": \"product_id\"\n",
    "        },\n",
    "        \"order_item_id\":\"INT\",\n",
    "        \"order_id\": \"INT\",\n",
    "        \"product_id\": \"INT\",\n",
    "        \"quantity\": \"INT\",\n",
    "        \"unit_price\": \"DECIMAL(10, 2)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "req = {\n",
    "    \"products\": [\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"price\"\n",
    "    ],\n",
    "    \"customers\": [\n",
    "        \"customer_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "        \"email\"\n",
    "    ],\n",
    "    \"orders\": [\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_date\",\n",
    "        \"total_amount\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic COUNT query\n",
    "q1 = \"What is the total number of customers?\"\n",
    "sql1 = \"SELECT COUNT(*) FROM customers;\"\n",
    "\n",
    "# Simple SELECT with WHERE clause\n",
    "q2 = \"List all products names with a price greater than $50.\"\n",
    "sql2 = \"SELECT product_name, price FROM products WHERE price > 50;\"\n",
    "\n",
    "# JOIN operation with ORDER BY\n",
    "q3 = \"Show the first name of customers who have placed orders, sorted by their last name.\"\n",
    "sql3 = \"\"\"\n",
    "SELECT DISTINCT c.first_name, c.last_name \n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "ORDER BY c.last_name;\n",
    "\"\"\"\n",
    "\n",
    "# Aggregation with GROUP BY\n",
    "q4 = \"What is the total amount of orders for each customer?\"\n",
    "sql4 = \"\"\"\n",
    "SELECT c.customer_id, c.first_name, c.last_name, SUM(o.total_amount) as total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "GROUP BY c.customer_id, c.first_name, c.last_name;\n",
    "\"\"\"\n",
    "\n",
    "# HAVING clause\n",
    "q5 = \"Which categories have more than 5 products?\"\n",
    "sql5 = \"\"\"\n",
    "SELECT c.category_name, COUNT(p.product_id) as product_count\n",
    "FROM categories c\n",
    "JOIN products p ON c.category_id = p.category_id\n",
    "GROUP BY c.category_name\n",
    "HAVING COUNT(p.product_id) > 5;\n",
    "\"\"\"\n",
    "\n",
    "# LIMIT clause\n",
    "q6 = \"What are the top 5 most expensive products?\"\n",
    "sql6 = \"\"\"\n",
    "SELECT product_name, price\n",
    "FROM products\n",
    "ORDER BY price DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "# Aggregation in WHERE clause\n",
    "q7 = \"Find all orders with more than 3 items.\"\n",
    "sql7 = \"\"\"\n",
    "SELECT o.order_id, COUNT(oi.product_id) as item_count\n",
    "FROM orders o\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "GROUP BY o.order_id\n",
    "HAVING COUNT(oi.product_id) > 3;\n",
    "\"\"\"\n",
    "\n",
    "# Multiple JOINs\n",
    "q8 = \"List the top 3 customers who have spent the most on 'Electronics' category products.\"\n",
    "sql8 = \"\"\"\n",
    "SELECT c.customer_id, c.first_name, c.last_name, SUM(oi.quantity * oi.unit_price) as total_spent\n",
    "FROM customers c\n",
    "JOIN orders o ON c.customer_id = o.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "JOIN categories cat ON p.category_id = cat.category_id\n",
    "WHERE cat.category_name = 'Electronics'\n",
    "GROUP BY c.customer_id, c.first_name, c.last_name\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 3;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'the',\n",
    "    'i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'we', 'our', 'ours', 'ourselves', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'do', 'does', 'did', 'have', 'has', 'had', 'can', 'could', 'shall', 'should', 'will', 'would', 'may', 'might', 'must',\n",
    "    'about', 'across', 'after', 'against', 'along', 'around', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'during', 'into', 'near', 'off', 'out', 'through', 'toward', 'under', 'up', 'with',\n",
    "    'if', 'then', 'because', 'as', 'until', 'while',\n",
    "    'this', 'that', 'these', 'those', 'such', 'what', 'which', 'whose', 'whoever', 'whatever', 'whichever', 'whomever', 'either', 'neither', 'both',\n",
    "    'very', 'really', 'always', 'never', 'too', 'already', 'often', 'sometimes', 'rarely', 'seldom', 'again', 'further', 'then', 'once', 'here', 'there', 'where', 'why', 'how',\n",
    "    # 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'second', 'third', 'fourth', 'fifth', # find a way to implement it in limit\n",
    "    'few', 'little', 'much', 'enough',\n",
    "    'yes', 'no', 'not', 'okay', 'ok', 'right', 'sure', 'well', 'uh', 'um', 'oh', 'eh', 'hmm', 'just', 'ever', 'yet', 'etc', 'perhaps', 'maybe', 'list'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aggregate_functions = {\n",
    "    'COUNT': ['count', 'number of', 'quantity of', 'total number of', 'tally', 'enumerate', 'how many'],\n",
    "    'SUM': ['sum', 'total', 'aggregate', 'combined', 'add up', 'overall', 'cumulative'],\n",
    "    'AVG': ['average', 'mean', 'typical', 'median', 'expected value', 'norm', 'central tendency'],\n",
    "    'MAX': ['maximum', 'highest', 'most', 'top', 'peak', 'greatest', 'largest', 'biggest', 'uppermost'],\n",
    "    'MIN': ['minimum', 'lowest', 'least', 'bottom', 'smallest', 'tiniest', 'least', 'floor'],\n",
    "    'DISTINCT': ['unique', 'different', 'distinct', 'individual', 'separate', 'non-duplicate', 'exclusive'],\n",
    "    'GROUP_CONCAT': ['concatenate', 'combine strings', 'join', 'merge text', 'string aggregation', 'text combination'],\n",
    "    # 'FIRST': ['first', 'initial', 'earliest', 'primary', 'leading', 'opening', 'foremost'], # limit implementation along with number\n",
    "    # 'LAST': ['last', 'final', 'latest', 'ultimate', 'concluding', 'terminal', 'closing']\n",
    "}\n",
    "\n",
    "comparison_operators = {\n",
    "    '>': ['greater than', 'more than', 'above', 'over', 'exceeding', 'surpassing', 'beyond', 'higher than', 'in excess of'],\n",
    "    '<': ['less than', 'fewer than', 'below', 'under', 'beneath', 'lower than', 'not as much as', 'smaller than'],\n",
    "    '=': ['equal to', 'same as', 'identical to', 'matching', 'equivalent to', 'corresponds to', 'is', 'for'],\n",
    "    '!=': ['not equal to', 'different from', 'excluding', 'not the same as', 'dissimilar to', 'unlike', 'other than'],\n",
    "    '>=': ['greater than or equal to', 'at least', 'no less than', 'minimum of', 'not below', ' starting from'],\n",
    "    '<=': ['less than or equal to', 'at most', 'no more than', 'maximum of', 'not above', 'up to'],\n",
    "    'BETWEEN': ['between', 'in the range of', 'within the bounds of', 'inside the limits of'],\n",
    "    'IN': ['in', 'within', 'among', 'included in', 'part of', 'contained in', 'one of'],\n",
    "    'NOT IN': ['not in', 'outside of', 'excluded from', 'not among', 'not part of', 'not contained in'],\n",
    "    'LIKE': ['like', 'similar to', 'resembling', 'matching pattern', 'corresponding to'],\n",
    "    'NOT LIKE': ['not like', 'dissimilar to', 'unlike', 'not matching pattern', 'different from pattern']\n",
    "}\n",
    "\n",
    "logical_operators = {\n",
    "    'AND': ['and', 'also', 'as well as', 'in addition to', 'plus', 'together with', 'along with', 'including'],\n",
    "    'OR': ['or', 'alternatively', 'either', 'otherwise', 'else', 'and/or'],\n",
    "    'NOT': ['not', 'except', 'excluding', 'other than', 'but not', 'save for', 'apart from']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = {\n",
    "    'INTEGER': ['integer', 'int', 'whole number', 'numeric'],\n",
    "    'FLOAT': ['float', 'decimal', 'real number', 'fractional number'],\n",
    "    'VARCHAR': ['string', 'text', 'characters', 'alphanumeric'],\n",
    "    'DATE': ['date', 'calendar day', 'day'],\n",
    "    'TIMESTAMP': ['timestamp', 'date and time', 'moment', 'point in time'],\n",
    "    'BOOLEAN': ['boolean', 'true/false', 'yes/no', 'binary']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order matters\n",
    "sql_clauses = {\n",
    "    'FROM': ['from', 'in', 'out of', 'sourced from', 'derived from', 'based on'],\n",
    "    'WHERE': ['where', 'for which', 'that have', 'meeting the condition', 'satisfying', 'fulfilling'],\n",
    "    'GROUP BY': ['group by', 'categorize by', 'classify by', 'organize by', 'arrange by', 'cluster by', 'for each', 'broken down by', 'per', 'by'], # check about 'by'\n",
    "    'HAVING': ['having', 'with the condition', 'subject to', 'meeting the criteria'],\n",
    "    'ORDER BY': ['order by', 'sort by', 'arrange by', 'rank by', 'sequence by'],\n",
    "    'LIMIT': ['limit', 'top', 'first', 'restrict to', 'cap at', 'only show'],\n",
    "    'JOIN': ['join', 'combine', 'merge', 'connect', 'link', 'associate'],\n",
    "    'UNION': ['union', 'combine', 'merge', 'incorporate', 'consolidate', 'unite'],\n",
    "    'INTERSECT': ['intersect', 'in common', 'shared by', 'mutual', 'overlapping'],\n",
    "    'EXCEPT': ['except', 'subtract', 'exclude', 'remove', 'omit', 'leave out'],\n",
    "    'SELECT': ['show', 'list all', 'list', 'give', 'return', 'fetch', 'retrieve', 'get', 'find', 'which', 'what is the', 'what is', 'what are', 'what'], # 'display\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_KEYWORDS = set(list(aggregate_functions.keys())) | (set(list(logical_operators.keys()))) | (set(sql_clauses.keys())) | (set(comparison_operators.keys()))\n",
    "ALL_KEYWORDS = list(map(str.lower, ALL_KEYWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not',\n",
       " '<',\n",
       " 'count',\n",
       " 'having',\n",
       " 'avg',\n",
       " 'and',\n",
       " 'join',\n",
       " 'max',\n",
       " 'select',\n",
       " 'not like',\n",
       " 'min',\n",
       " 'union',\n",
       " 'like',\n",
       " '>=',\n",
       " 'order by',\n",
       " 'limit',\n",
       " 'from',\n",
       " 'not in',\n",
       " 'distinct',\n",
       " 'except',\n",
       " 'or',\n",
       " '=',\n",
       " 'between',\n",
       " 'last',\n",
       " '!=',\n",
       " 'intersect',\n",
       " 'group by',\n",
       " 'first',\n",
       " 'where',\n",
       " '>',\n",
       " 'in',\n",
       " 'sum',\n",
       " 'group_concat',\n",
       " '<=']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\"ALL_KEYWORDS\": ALL_KEYWORDS,\n",
    "             \"sql_clauses\": sql_clauses,\n",
    "             \"data_types\": data_types,\n",
    "             \"logical_operators\": logical_operators,\n",
    "             \"comparison_operators\": comparison_operators,\n",
    "             \"aggregate_functions\": aggregate_functions,\n",
    "             \"stop_words\": stop_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_string(text, replace_dict):\n",
    "    # Create a reverse mapping from values to keys\n",
    "    value_to_key = {}\n",
    "    for key, values in replace_dict.items():\n",
    "        for value in values:\n",
    "            value_to_key[value.lower()] = key.lower()\n",
    "    \n",
    "    # Create a regex pattern for matching words\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(word) for word in value_to_key.keys()) + r')\\b'\n",
    "    \n",
    "    # Function to replace matched words\n",
    "    def replace_word(match):\n",
    "        return value_to_key.get(match.group(0).lower(), match.group(0))\n",
    "    \n",
    "    # Perform the replacement\n",
    "    replaced_text = re.sub(pattern, replace_word, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return replaced_text\n",
    "\n",
    "def replace_numbers(token):\n",
    "    try:\n",
    "        return w2n.word_to_num(token)\n",
    "    except:\n",
    "        return token\n",
    "\n",
    "def preprocess_text(text, constants):\n",
    "    # Lowercase\n",
    "    # text = text.lower() # change it to lower while comparisson\n",
    "    \n",
    "    # Remove punctuation\n",
    "    # pattern = r'[^\\w\\s()*\\d-]|(?<!\\d)-(?!\\d)'\n",
    "    # text = re.sub(pattern, '', text)\n",
    "    text = re.sub(r'\\?', '', text)\n",
    "    text = re.sub(r'\\$', '', text)\n",
    "    text = re.sub(r'.$', '', text)\n",
    "    text = re.sub(r',', '', text)\n",
    "    pattern = r'\\b(\\d+)-(\\d+)\\b'\n",
    "    text = re.sub(pattern, r'\\1 to \\2', text)\n",
    "\n",
    "    # substitute words\n",
    "    text = replace_string(text, sql_clauses)\n",
    "    text = replace_string(text, aggregate_functions)\n",
    "    # from \\d+ -> >=\n",
    "    text = replace_string(text, comparison_operators)\n",
    "    between_pattern = r'from \\b(\\w+|\\d+)\\b to \\b(\\w+|\\d+)\\b | \\b(\\w+|\\d+)\\b to \\b(\\w+|\\d+)\\b'\n",
    "    text = re.sub(between_pattern, r\" between(\\1, \\2)\", text)\n",
    "    text = replace_string(text, logical_operators)\n",
    "\n",
    "    \n",
    "\n",
    "    # Tokenization\n",
    "    # tokens = word_tokenize(text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # \"name\" condition\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens) - 1:  # Changed to len(tokens) - 1\n",
    "        if tokens[i].lower() not in constants['ALL_KEYWORDS'] and (tokens[i+1].lower() == \"name\" or tokens[i+1].lower() == \"names\"):\n",
    "            # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    if i < len(tokens):  # Add any remaining token\n",
    "        result.append(tokens[i])\n",
    "\n",
    "    # print(result)\n",
    "    tokens = result\n",
    "\n",
    "    tokens = [str(replace_numbers(token)) for token in tokens]\n",
    "    print(tokens)\n",
    "\n",
    "    # Remove stopwords\n",
    "    #  load custom stop words\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatization (optional) // important for similarity\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens# ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select', 'count', 'customer']\n",
      "['select', 'products_names', 'with', 'a', 'price', '>', '50']\n",
      "['select', 'sum', 'amount', 'of', 'orders', 'group', 'by', 'custome']\n",
      "What is the total number of customers?\n",
      "List all products names with a price greater than $50.\n",
      "What is the total amount of orders for each customer?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['select', 'count', 'customer'],\n",
       " ['select', 'products_names', 'price', '>', '50'],\n",
       " ['select', 'sum', 'amount', 'of', 'order', 'group', 'by', 'custome'])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_tok = preprocess_text(q1, constants)\n",
    "q2_tok = preprocess_text(q2, constants)\n",
    "q4_tok = preprocess_text(q4, constants)\n",
    "print(q1)\n",
    "print(q2)\n",
    "print(q4)\n",
    "q1_tok, q2_tok, q4_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keywords(tokens, keywords):\n",
    "    token_without_key = [token for token in tokens if token not in keywords]\n",
    "\n",
    "    return token_without_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['customer'],\n",
       " ['products_names', 'price', '50'],\n",
       " ['amount', 'of', 'order', 'group', 'by', 'custome'])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_nk_tok = remove_keywords(q1_tok, ALL_KEYWORDS)\n",
    "q2_nk_tok = remove_keywords(q2_tok, ALL_KEYWORDS)\n",
    "q4_nk_tok = remove_keywords(q4_tok, ALL_KEYWORDS)\n",
    "q1_nk_tok, q2_nk_tok, q4_nk_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_group_by(tokens, schema):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens) - 2:  # Changed to len(tokens) - 1\n",
    "\n",
    "        if tokens[i].lower() == \"group\" and tokens[i+1].lower() == \"by\":\n",
    "            # print(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            for table, columns in schema.items():\n",
    "                for column in schema[table].keys():\n",
    "                # for column in columns.keys():\n",
    "                    if column=='pk' or column=='fk':\n",
    "                        continue\n",
    "                    else:\n",
    "                        match = get_close_matches(tokens[i+2], columns)\n",
    "                        if match:\n",
    "                            # print(tokens[i+2], \":\", match)                        \n",
    "                            result.append(f\"{tokens[i]} {tokens[i+1]} {match[0]}\")\n",
    "            i += 3\n",
    "        else:\n",
    "            result.append(tokens[i])\n",
    "            i += 1\n",
    "    if i < len(tokens):  # Add any remaining token\n",
    "        result.extend(tokens[i:])\n",
    "    \n",
    "    return list(dict.fromkeys(result)) # removing duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total number of customers?\n",
      "['customer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['customer']"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q1)\n",
    "q1_nk_tok = remove_keywords(q1_tok, ALL_KEYWORDS)\n",
    "print(q1_nk_tok)\n",
    "group_tok1 = identify_group_by(q1_nk_tok, SQL)\n",
    "group_tok1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all products names with a price greater than $50.\n",
      "['products_names', 'price', '50']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['products_names', 'price', '50']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q2)\n",
    "q2_nk_tok = remove_keywords(q2_tok, ALL_KEYWORDS)\n",
    "print(q2_nk_tok)\n",
    "group_tok2 = identify_group_by(q2_nk_tok, SQL)\n",
    "group_tok2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"products_names\", \"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total amount of orders for each customer?\n",
      "['amount', 'of', 'order', 'group', 'by', 'custome']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amount', 'of', 'order', 'group by customer_id']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q4)\n",
    "q4_nk_tok = remove_keywords(q4_tok, ALL_KEYWORDS)\n",
    "print(q4_nk_tok)\n",
    "group_tok4 = identify_group_by(q4_nk_tok, SQL)\n",
    "group_tok4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Columns and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by before this\n",
    "def indentify_table(tokens, schema):\n",
    "    \n",
    "    detected_table = [[], [], []] # [[table names], [similarities], [tokens]]\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() == \"from\":\n",
    "            table_name = tokens[i + 1]\n",
    "            if table_name in schema:\n",
    "                print(f\"Table identified: {table_name}\")\n",
    "                return table_name\n",
    "        \n",
    "        for table in schema.keys():\n",
    "            similarity = fuzz.ratio(token, table)\n",
    "            if token in detected_table[2]:\n",
    "                # print(f\"Table identified: {token} {table} {similarity}\")\n",
    "                # print(detected_table)\n",
    "                i = detected_table[2].index(token)\n",
    "                if similarity >= detected_table[1][i]:\n",
    "                    # detected_table[0] = table\n",
    "                    detected_table[1][i] = similarity\n",
    "                    detected_table[0][i] = table\n",
    "                # print(f\"Table identified: {table} {similarity}\")\n",
    "            else:\n",
    "                if similarity >= 70: # initial threshold\n",
    "                    detected_table[0].append(table)\n",
    "                    detected_table[1].append(similarity)\n",
    "                    detected_table[2].append(token)\n",
    "\n",
    "\n",
    "    for i in range(len(detected_table[0])):\n",
    "        if detected_table[1][i] > 85:\n",
    "            tokens.remove(detected_table[2][i])    \n",
    "        \n",
    "    # print(\"dsfsdgd\",detected_table[0])\n",
    "\n",
    "    return detected_table[0]\n",
    "\n",
    "\n",
    "def indentify_col_tables(tokens, schema):\n",
    "    res = dict()\n",
    "\n",
    "    identified_table = indentify_table(tokens, schema)\n",
    "    print(identified_table)\n",
    "    if not tokens:\n",
    "        res[identified_table[0]] = ['*']\n",
    "        return res\n",
    "\n",
    "    if identified_table:\n",
    "        for table in identified_table:\n",
    "            res[table] = dict()\n",
    "            for token in tokens:\n",
    "                for column in schema[table].keys():\n",
    "                    # for column in columns.keys():\n",
    "                    if column=='pk' or column=='fk':\n",
    "                        continue\n",
    "                    else:\n",
    "                        similarity = fuzz.ratio(token, column)\n",
    "                        if similarity >= 50:\n",
    "                            if res[table].get(token):\n",
    "                                # print(\"case 1\")\n",
    "                                # print(token, \":\", column, similarity)\n",
    "                                old_similarity = list(res[table][token].values())[0]\n",
    "                                # print(\"sim\", old_similarity)\n",
    "                                if similarity > old_similarity:\n",
    "                                    res[table][token] = {column: similarity}\n",
    "                            else:\n",
    "                                # print(\"case 2\")\n",
    "                                # print(token, \":\", column, similarity)\n",
    "                                res[table][token] = {column: similarity}\n",
    "                            # print(res)\n",
    "                            # if res.get(identified_table):\n",
    "                            #     res[identified_table].append(column)\n",
    "                            # else:\n",
    "                            #     res[identified_table] = [column]\n",
    "    else:\n",
    "        for token in tokens:\n",
    "            for table, columns in schema.items():\n",
    "                for column in schema[table].keys():\n",
    "                # for column in columns.keys():\n",
    "                    if column=='pk' or column=='fk':\n",
    "                        continue\n",
    "                    else:\n",
    "                        match = get_close_matches(token, columns)\n",
    "                        if match:\n",
    "                            print(\"*\"*4, token, \":\", match[0])\n",
    "                            # if token in columns: # replace by fuzzy logic\n",
    "                            # print(f\"Column identified: {column} in table {table}\")\n",
    "                            if res.get(table):\n",
    "                                res[table].append(match[0])\n",
    "                            else:\n",
    "                                res[table] = [match[0]]\n",
    "\n",
    "    if identified_table:\n",
    "        result = {}\n",
    "        for table, token in res.items():\n",
    "            cols = []\n",
    "            for col in token.values():\n",
    "                cols.extend(col.keys())\n",
    "            result[table] = cols\n",
    "\n",
    "        result = {key: value for key, value in result.items() if value != []}\n",
    "        return result\n",
    "    \n",
    "\n",
    "    res = {key: value for key, value in res.items() if value != []}    \n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"unit_price\", \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all products names with a price greater than $50.\n",
      "['products_names', 'price', '50']\n",
      "['products']\n",
      "['products']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'products': ['product_name', 'price']}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q2)\n",
    "print(q2_nk_tok)\n",
    "q2_nk_tok = remove_keywords(q2_tok, ALL_KEYWORDS)\n",
    "req = indentify_col_tables(group_tok2, SQL)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total number of customers?\n",
      "['customer']\n",
      "dsfsdgd ['customers']\n",
      "['customers']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'customers': ['*']}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q1)\n",
    "print(q1_nk_tok)\n",
    "group_tok1 = identify_group_by(q1_nk_tok, SQL)\n",
    "req = indentify_col_tables(group_tok1, SQL)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total amount of orders for each customer?\n",
      "['amount', 'of', 'order', 'group', 'by', 'custome']\n",
      "['orders']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'orders': ['total_amount', 'customer_id']}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q4)\n",
    "q4_nk_tok = remove_keywords(q4_tok, ALL_KEYWORDS)\n",
    "print(q4_nk_tok)\n",
    "\n",
    "group_tok4 = identify_group_by(q4_nk_tok, SQL)\n",
    "req = indentify_col_tables(group_tok4, SQL)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indentify_table(q1_nk_tok, SQL)\n",
    "fuzz.ratio(\"group by customer_id\", \"customer_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(db_schema, directional=False):\n",
    "    if directional:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "\n",
    "    # Add nodes and edges based on the SQL dictionary\n",
    "    for table, details in db_schema.items():\n",
    "        G.add_node(table)  # Add table as a node\n",
    "        for fk_table in details['fk']:  # Iterate through foreign keys\n",
    "            G.add_edge(table, fk_table)  # Create an edge from current table to foreign key table\n",
    "\n",
    "    # Get nodes and edges for verification\n",
    "    nodes = list(G.nodes)\n",
    "    edges = list(G.edges)\n",
    "\n",
    "    # Output nodes and edges\n",
    "    # print(nodes)\n",
    "    # print(edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def required_tables_graph(G, start, end, required_tables):\n",
    "    def dijkstra(graph, start, end):\n",
    "        distances = {node: float('infinity') for node in graph}\n",
    "        distances[start] = 0\n",
    "        pq = [(0, start)]\n",
    "        previous = {node: None for node in graph}\n",
    "\n",
    "        while pq:\n",
    "            current_distance, current_node = heapq.heappop(pq)\n",
    "\n",
    "            if current_node == end:\n",
    "                path = []\n",
    "                while current_node:\n",
    "                    path.append(current_node)\n",
    "                    current_node = previous[current_node]\n",
    "                return path[::-1], current_distance\n",
    "\n",
    "            for neighbor in graph[current_node]:\n",
    "                distance = current_distance + 1  # All edges have weight of 1\n",
    "                if distance < distances[neighbor]:\n",
    "                    distances[neighbor] = distance\n",
    "                    previous[neighbor] = current_node\n",
    "                    heapq.heappush(pq, (distance, neighbor))\n",
    "\n",
    "        return None, float('infinity')\n",
    "\n",
    "    required_tables = set(required_tables) - {start, end}\n",
    "    best_path = None\n",
    "    best_distance = float('infinity')\n",
    "\n",
    "    def dfs(current_path, current_distance, remaining_required):\n",
    "        nonlocal best_path, best_distance\n",
    "\n",
    "        if not remaining_required:\n",
    "            path, distance = dijkstra(G, current_path[-1], end)\n",
    "            if path:\n",
    "                total_path = current_path + path[1:]\n",
    "                total_distance = current_distance + distance\n",
    "                if total_distance < best_distance:\n",
    "                    best_path = total_path\n",
    "                    best_distance = total_distance\n",
    "            return\n",
    "\n",
    "        for node in remaining_required:\n",
    "            path, distance = dijkstra(G, current_path[-1], node)\n",
    "            if path:\n",
    "                new_path = current_path + path[1:]\n",
    "                new_distance = current_distance + distance\n",
    "                new_remaining = remaining_required - {node}\n",
    "                dfs(new_path, new_distance, new_remaining)\n",
    "\n",
    "    dfs([start], 0, required_tables)\n",
    "\n",
    "    return best_path, best_distance\n",
    "\n",
    "# G = create_graph(SQL, directional=False)\n",
    "# # Set start, end, and required nodes\n",
    "# required_tables = ['categories', 'products', 'orders']\n",
    "# start = required_tables[0]\n",
    "# end = required_tables[2]\n",
    "\n",
    "# # Find the shortest path\n",
    "# required_tables, distance = required_tables_graph(G, start, end, required_tables)\n",
    "\n",
    "# print(f\"Shortest path: {' -> '.join(required_tables) if required_tables else 'No path found'}\")\n",
    "# # Shortest path: categories -> products -> order_items -> orders\n",
    "\n",
    "# print(f\"Total distance: {distance}\")\n",
    "# # Total distance: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('products', 'categories'),\n",
       " ('orders', 'customers'),\n",
       " ('order_items', 'orders'),\n",
       " ('order_items', 'products')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = create_graph(SQL, directional=True)\n",
    "list(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_sort(edges):\n",
    "    counts = Counter(t[0] for t in edges)\n",
    "    \n",
    "    # Sort tuples based on the counts in descending order\n",
    "    sorted_tuples = sorted(edges, key=lambda x: counts[x[0]], reverse=True)\n",
    "    \n",
    "    return sorted_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_items', 'orders'),\n",
       " ('order_items', 'products'),\n",
       " ('products', 'categories'),\n",
       " ('orders', 'customers')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_sort(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_clause(req_schema: dict, db_schema: dict):\n",
    "    clause = \"\"\n",
    "\n",
    "    # not required\n",
    "    # max_cols = 0\n",
    "    # prim_table = None\n",
    "    # # change logic based on foreign key numbers\n",
    "    # for table, cols in req_schema.items():\n",
    "    #     if len(cols) > max_cols:\n",
    "    #         max_cols = len(cols)\n",
    "    #         prim_table = table\n",
    "    \n",
    "    # # pk_col = db_schema[prim_table]['pk']\n",
    "    # print(prim_table)\n",
    "\n",
    "    # create graph from Database schema\n",
    "    db_graph = create_graph(db_schema)\n",
    "    db_dir_graph = create_graph(db_schema, directional=True)\n",
    "\n",
    "    # print(db_dir_graph)\n",
    "\n",
    "    required_tables = list(req_schema.keys())\n",
    "\n",
    "    print(required_tables)\n",
    "\n",
    "    min_dist = float('inf')\n",
    "    for st_table in required_tables:\n",
    "        for end_table in required_tables:\n",
    "            sub_graph, distance = required_tables_graph(db_graph, st_table, end_table, required_tables)\n",
    "            if distance < min_dist:\n",
    "                join_graph = OrderedSet(sub_graph)\n",
    "                min_dist = distance\n",
    "\n",
    "    print(join_graph)\n",
    "\n",
    "    req_graph = []\n",
    "\n",
    "    for edge in db_dir_graph.edges:\n",
    "        # assumption binary relations between tables\n",
    "        if edge[0] in join_graph and edge[1] in join_graph:\n",
    "            req_graph.append(edge)\n",
    "\n",
    "    req_graph = graph_sort(req_graph)\n",
    "    print(\"required graph: \", req_graph)\n",
    "\n",
    "    clause += f\"FROM {req_graph[0][0]}\\n\"\n",
    "\n",
    "    for i, edge in enumerate(req_graph):\n",
    "        table1 = edge[0]\n",
    "        table2 = edge[1]\n",
    "        print(edge)\n",
    "        # if table1 in db_schema[table2]['fk']:\n",
    "        #     # print(\"1\")\n",
    "        #     fk_col = db_schema[table2]['fk'][table1] # foreign key corresponding to 2nd table  primary key\n",
    "        #     pk_col = db_schema[table1]['pk'][0] # primary key of 2nd table\n",
    "        #     clause += f\"JOIN {table1} ON {table2}.{fk_col}={table1}.{pk_col} \\n\"\n",
    "        # else:\n",
    "        fk_col = db_schema[table1]['fk'][table2] # foreign key corresponding to 2nd table  primary key\n",
    "        pk_col = db_schema[table2]['pk'][0] # primary key of 2nd table\n",
    "        clause += f\"JOIN {table2} ON {table1}.{fk_col}={table2}.{pk_col} \\n\"\n",
    "        \n",
    "    return clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orders': ['total_amount'], 'customers': []}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orders', 'customers']\n",
      "OrderedSet(['orders', 'customers'])\n",
      "required graph:  [('orders', 'customers')]\n",
      "('orders', 'customers')\n",
      "FROM orders\n",
      "JOIN customers ON orders.customer_id=customers.customer_id \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if len(req.keys()) > 1:\n",
    "    print(join_clause(req, SQL))\n",
    "else:\n",
    "    print(\"No Join required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['books', 'authors']\n",
      "OrderedSet(['books', 'authors'])\n",
      "required graph:  [('books', 'authors')]\n",
      "('books', 'authors')\n",
      "FROM books\n",
      "JOIN authors ON books.author_id=authors.author_id \n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQL2 = {\n",
    "    \"books\": {\n",
    "        \"pk\": [\"book_id\"],\n",
    "        \"fk\": {\"authors\": \"author_id\"},\n",
    "        \"book_id\": \"INT\",\n",
    "        \"title\": \"VARCHAR(255)\",\n",
    "        \"author_id\": \"INT\",\n",
    "        \"isbn\": \"VARCHAR(13)\",\n",
    "        \"publication_year\": \"INT\",\n",
    "        \"price\": \"DECIMAL(10, 2)\",\n",
    "        \"stock_quantity\": \"INT\"\n",
    "    },\n",
    "    \"authors\": {\n",
    "        \"pk\": [\"author_id\"],\n",
    "        \"fk\": {},\n",
    "        \"author_id\": \"INT\",\n",
    "        \"first_name\": \"VARCHAR(50)\",\n",
    "        \"last_name\": \"VARCHAR(50)\",\n",
    "        \"birth_date\": \"DATE\",\n",
    "        \"nationality\": \"VARCHAR(50)\",\n",
    "        \"biography\": \"TEXT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "req2 = {\n",
    "    \"books\": [\n",
    "        \"book_id\",\n",
    "        \"title\",\n",
    "        \"stock_quantity\"\n",
    "    ],\n",
    "    \"authors\": [\n",
    "        \"author_id\",\n",
    "        \"first_name\",\n",
    "        \"last_name\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(join_clause(req2, SQL2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the total number of customers?\n",
      "['select', 'count', 'customer']\n",
      "List all products names with a price greater than $50.\n",
      "products_names\n",
      "['select', 'products_names', 'price', '>', '50']\n",
      "Show the first name of customers who have placed orders, sorted by their last name.\n",
      "['select', 'limit', 'name', 'of', 'customer', 'who', 'placed', 'order', 'sorted', 'group', 'by', 'last', 'name']\n",
      "What is the total amount of orders for each customer?\n",
      "['select', 'sum', 'amount', 'of', 'order', 'group', 'by', 'custome']\n",
      "Which categories have more than 5 products?\n",
      "['select', 'category', '>', '5', 'product']\n",
      "What are the top 5 most expensive products?\n",
      "['select', 'limit', '5', 'max', 'expensive', 'product']\n",
      "Find all orders with more than 3 items.\n",
      "['select', 'all', 'order', '>', '3', 'item']\n",
      "List the top 3 customers who have spent the most on 'Electronics' category products.\n",
      "['select', 'limit', '3', 'customer', 'who', 'spent', 'max', 'on', \"'Electronics'\", 'category', 'product']\n"
     ]
    }
   ],
   "source": [
    "print(q1)\n",
    "print(preprocess_text(q1, constants))\n",
    "print((q2))\n",
    "print(preprocess_text(q2, constants))\n",
    "print((q3))\n",
    "print(preprocess_text(q3, constants))\n",
    "print((q4))\n",
    "print(preprocess_text(q4, constants))\n",
    "print((q5))\n",
    "print(preprocess_text(q5, constants))\n",
    "print((q6))\n",
    "print(preprocess_text(q6, constants))\n",
    "print((q7))\n",
    "print(preprocess_text(q7, constants))\n",
    "print((q8))\n",
    "print(preprocess_text(q8, constants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# difference between where and having\n",
    "def identify_where_condition(tokens, req):\n",
    "    conditions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() == \"where\":\n",
    "            conditions.append(\" \".join(tokens[i+1:])) # refine with \n",
    "            break\n",
    "\n",
    "    print(f\"Conditions: {conditions}\")\n",
    "    # Example Output: Conditions: [\"age > 30\"]\n",
    "\n",
    "\n",
    "def indentify_having_condition(tokens):\n",
    "    # need to check for aggregate functions in tokens\n",
    "    conditions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() == \"having\":\n",
    "            conditions.append(\" \".join(tokens[i+1:])) # refine with \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
